(window.webpackJsonp=window.webpackJsonp||[]).push([[33],{399:function(t,a,r){"use strict";r.r(a);var n=r(42),s=Object(n.a)({},(function(){var t=this,a=t.$createElement,r=t._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"rnn"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#rnn"}},[t._v("#")]),t._v(" RNN")]),t._v(" "),r("h2",{attrs:{id:"introduction"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),r("p",[t._v("RNN，Recurrent Neural Network，顾名思义，就是循环神经网络，在有CNN的情况下，为什么我们还需要RNN呢？原因就是CNN并不能很好地去处理时序数据，比如文章、语音等，而且缺乏对不同时长的数据的处理能力。因此，为了可以处理变长的时序数据和特征，提出了RNN。而最为传统的RNN结构就是我们熟知的"),r("strong",[t._v("Vanilla RNN")]),t._v("。")]),t._v(" "),r("p",[t._v("另外，RNN是一种语言模型，它考虑到当前时刻t之前的所有时刻(1,2...,t-1)")]),t._v(" "),r("h2",{attrs:{id:"loss-function"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#loss-function"}},[t._v("#")]),t._v(" loss function")]),t._v(" "),r("p",[t._v("RNN的损失函数采用的是Cross Entropy Loss，相当于做分类任务。")]),t._v(" "),r("h2",{attrs:{id:"与hmm的区别"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#与hmm的区别"}},[t._v("#")]),t._v(" 与HMM的区别")]),t._v(" "),r("p",[t._v("在RNN中，我们是通过观测数据传入隐含层，而HMM则是从隐含层传到观测值。")]),t._v(" "),r("p",[t._v("另外，在隐层表达中，RNN的是分布式表示，而HMM是one-hot表示。")]),t._v(" "),r("h2",{attrs:{id:"与cnn的区别"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#与cnn的区别"}},[t._v("#")]),t._v(" 与CNN的区别")]),t._v(" "),r("p",[t._v("两者的角度不同，CNN可以通过多个卷积层、池化层叠加，对数据的处理是在空间的维度上处理，而RNN在时间维度上算是deep model，但是纵向上来说，只能算是浅层神经网络。")]),t._v(" "),r("h2",{attrs:{id:"缺点"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#缺点"}},[t._v("#")]),t._v(" 缺点")]),t._v(" "),r("ol",[r("li",[r("p",[t._v("梯度消失")]),t._v(" "),r("p",[t._v("RNN采用BPTT(Backward Propagation Through Time)的方法对网络的参数进行优化，就算与传统神经网络的BP形式上略有不同，但是本质上实际都是一样的，都是反向传递，chain rule的过程。而我们也知道，Chain rule由于是多个项相乘，所以当有个项小于1时，梯度的更新会特别小；当有个项大于1时，会导致梯度的更新特别大。因此会导致梯度消失和梯度爆炸的问题。")]),t._v(" "),r("p",[t._v("解决办法：没有很好且有效直观的方法去解决。")])]),t._v(" "),r("li",[r("p",[t._v("梯度爆炸")]),t._v(" "),r("p",[t._v("同上。解决办法有：Gradient Clipping。")])]),t._v(" "),r("li",[r("p",[t._v("Long-Term Dependency长时间依赖问题")]),t._v(" "),r("p",[t._v("因为梯度的问题，很难捕获到长文本的依赖（值太小）。")])])])])}),[],!1,null,null,null);a.default=s.exports}}]);