(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{357:function(t,a,s){t.exports=s.p+"assets/img/transformer.dfc59528.png"},358:function(t,a,s){t.exports=s.p+"assets/img/structure.188eba9d.png"},359:function(t,a,s){t.exports=s.p+"assets/img/self-attention.b1c69273.png"},406:function(t,a,s){"use strict";s.r(a);var e=s(42),i=Object(e.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"transformer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[t._v("#")]),t._v(" Transformer")]),t._v(" "),e("h2",{attrs:{id:"introduction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),e("p",[t._v("什么是Transformer？能否介绍一下Transformer？这是面试的时候常会问到的问题，关于这个问题，我的答案是这样的：")]),t._v(" "),e("p",[t._v("Transformer是一个基于Seq2Seq的Encoder-Decoder之上的深度神经网络，它用multi-head的self-Attention结构取代了原本Seq2seq之中采用的RNN或LSTM的结构，输入进模型的除了token的embedding之外，还有position embedding位置编码，该位置基于奇偶位置用sin、cos去计算位置编码的值。位置编码和word embedding输入模型的Encoder模块。Transformer的Encoder部分是由6个Encoder layer叠加而成，每个Encoder layer包括了一个self-attention layer和feed-forward layer组成，之后再传到下一个encoder layer去。其中，在self-attention和feed-forward 两个layer后都有一个add&norm，作残差相加和归一化的计算。之后Encoder输出的是一个语义向量，输入到Decoder模块中。Decoder模块同样是由6个decoder layer组成，而每个decoder layer与encoder layer稍微有些不同，每个decoder layer包括一个self-attention、multi-head encoder-decoder layer和feed-forward layer，其中self-attention用的是encoder部分中的K矩阵和V矩阵，另外用了一个attention mask，把还没轮到的token遮掩住。而encoder-decoder attention部分是和传统的attention计算方式是一样的。同样，decoder layer的每一part之间存在着add&norm模块。")]),t._v(" "),e("p",[e("img",{attrs:{src:s(357),alt:"Transformer"}})]),t._v(" "),e("p",[e("img",{attrs:{src:s(358),alt:"Encoder-Decoder"}})]),t._v(" "),e("h2",{attrs:{id:"self-attention-layer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#self-attention-layer"}},[t._v("#")]),t._v(" Self Attention layer")]),t._v(" "),e("p",[t._v("self attention，说是attention，实际上和attention并没啥关系。self attention关注的是输入中的token与token之间的潜在联系。self attention的计算方式如下，首先position embedding和word embedding相加后，作为输入进入到模型，与三个权重矩阵相乘，得到q，k，v三个向量，q与当前所有输入的k矩阵进行"),e("strong",[t._v("点积")]),t._v("计算（可以用其他相似度计算方法），之后将这些score进行softmax计算，作为每个token的在当前输入对当前token的意义的比例，也就是权重，之后将这个权重与其他词的v作sum product，得到该token的输出，进入后续的步骤。")]),t._v(" "),e("p",[e("img",{attrs:{src:s(359),alt:"self-attention"}})]),t._v(" "),e("p",[t._v("另外，self attention 是"),e("strong",[t._v("并行")]),t._v("输入其他单词的，比起rnn、lstm，更能捕获长时间序列的文本依赖关系。")]),t._v(" "),e("p",[t._v("还有一点要提的是，在softmax之前，对每个score都要除以"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msqrt",[e("mrow",[e("msub",[e("mi",[t._v("d")]),e("mi",[t._v("k")])],1)],1)],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\sqrt {d_k}")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.8572200000000001em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1.04em","vertical-align":"-0.18278em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"sqrt mord"},[e("span",{staticClass:"sqrt-sign",staticStyle:{top:"-0.017220000000000013em"}},[e("span",{staticClass:"style-wrap reset-textstyle textstyle uncramped"},[t._v("√")])]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),e("span",{staticClass:"mord textstyle cramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("d")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])]),e("span",{staticStyle:{top:"-0.77722em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle textstyle uncramped sqrt-line"})]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("，这表示的是每个embedding的维度，因为用多头注意力之后，原本的模型方差会由1变成更大的数值，论文中，实验通过除以这样一个数值，去缩小了方差。")]),t._v(" "),e("h2",{attrs:{id:"multi-head-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#multi-head-attention"}},[t._v("#")]),t._v(" Multi-head attention")]),t._v(" "),e("p",[t._v("从名字上可以看出，multihead，多个头。在这里的意思是，通过多个Q，K，V矩阵，完成self-attention的运算。计算完之后，横向将结果拼接起来，然后再乘以一个矩阵，进行降维，得到输出的向量。")]),t._v(" "),e("p",[t._v("比如说self-attention层输出的是一个1*d维的向量，经过了k个head，所以会得到一个1*kd的向量，这时候再乘以一个kd*d的矩阵，将向量变回1*d的维度。")]),t._v(" "),e("p",[t._v("**为什么要用multi-head attention？**这个也是面试的时候经常会被问到的一个问题，事实上，论文里也没有说这个的意义是什么，数学上也并没有详细的推理到，但是从直观意义上去理解，多个头可以关注到不同的子空间，增加了模型的泛化能力。")]),t._v(" "),e("h2",{attrs:{id:"position-embedding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#position-embedding"}},[t._v("#")]),t._v(" Position Embedding")]),t._v(" "),e("p",[t._v("由于训练Transformer模型时，序列输入采用并行方式，因此缺少单词的位置信息，通过在Transformer的输入中加入单词位置编码信息，使Transformer能够识别语句中单词的位置关系。")]),t._v(" "),e("p",[e("strong",[t._v("位置编码（positional encoding）")]),t._v("：位置编码向量与词向量维度相同，\\text{max_seq_len} \\times \\text{embedding_dim}。")]),t._v(" "),e("p",[t._v("Transformer原文中使用正、余弦函数的线性变换对单词位置编码：")]),t._v("\n\\text{PE}_{pos, 2i} = \\sin \\left( \\frac{pos}{10000^{2i / d_{\\text{model}}}} \\right) \\\\ \n\\text{PE}_{(pos,2i + 1)} = \\cos \\left( \\frac{pos}{10000^{2i / d_{\\text{model}}}} \\right)\n"),e("p",[t._v("其中，pos \\in [0, \\text{max_seq_len})表示单词在语句中的位置，i \\in [0, \\text{embedding_dim})表示词向量维度。位置编码函数的波长在"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mo",[t._v("[")]),e("mn",[t._v("2")]),e("mi",[t._v("π")]),e("mo",{attrs:{separator:"true"}},[t._v(",")]),e("mn",[t._v("1")]),e("mn",[t._v("0")]),e("mn",[t._v("0")]),e("mn",[t._v("0")]),e("mn",[t._v("0")]),e("mo",[t._v("×")]),e("mn",[t._v("2")]),e("mi",[t._v("π")]),e("mo",[t._v("]")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("[2 \\pi, 10000 \\times 2 \\pi]")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mopen"},[t._v("[")]),e("span",{staticClass:"mord mathrm"},[t._v("2")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("π")]),e("span",{staticClass:"mpunct"},[t._v(",")]),e("span",{staticClass:"mord mathrm"},[t._v("1")]),e("span",{staticClass:"mord mathrm"},[t._v("0")]),e("span",{staticClass:"mord mathrm"},[t._v("0")]),e("span",{staticClass:"mord mathrm"},[t._v("0")]),e("span",{staticClass:"mord mathrm"},[t._v("0")]),e("span",{staticClass:"mbin"},[t._v("×")]),e("span",{staticClass:"mord mathrm"},[t._v("2")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("π")]),e("span",{staticClass:"mclose"},[t._v("]")])])])]),t._v("区间内变化，语句中每一个单词位置沿词向量维度由周期不同的正、余弦函数交替取值组合，生成独一纹理信息，从而使模型学到位置间的依赖关系和自然语言的时序特性。")]),t._v(" "),e("h2",{attrs:{id:"decoder-attention-masking"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#decoder-attention-masking"}},[t._v("#")]),t._v(" Decoder attention masking")]),t._v(" "),e("p",[t._v("Decoder部分的self-attention layer，这里计算时候，不再用三个权重矩阵生成q、k、v三个向量，而是只需要一个Q矩阵计算出query向量即可，而K，V矩阵则是来自Encoder的K，V矩阵。")]),t._v(" "),e("p",[t._v("另外，self-attention是并行输入的，但是Decoder的生成步骤是一个接一个单词的输出，因此，需要将未出现的单词mask掉，操作很简单，将未出现的词语设置成负无穷的数即可，使其不产生作用。")]),t._v(" "),e("h2",{attrs:{id:"整体结构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#整体结构"}},[t._v("#")]),t._v(" 整体结构")]),t._v(" "),e("p",[t._v("Transformer编码器基本单元由两个子层组成：第一个子层实现多头**自注意力（self-attention）**机制（Multi-Head Attention）；第二个子层实现全连接前馈网络。计算过程如下：")]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("词向量与位置编码")])])]),t._v("\nX = \\text{EmbeddingLookup}(X) + \\text{PositionalEncoding} \\tag{2}\n\nX \\in \\mathbb{R}^{\\text{batch_size} \\times \\text{seq_len} \\times \\text{embedding_dim}}\n\n"),e("ol",{attrs:{start:"2"}},[e("li",[e("strong",[t._v("自注意力机制")])])]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("Q")]),e("mo",[t._v("=")]),e("mtext",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("L")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("n")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("a")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("r")])],1),e("mo",[t._v("(")]),e("mi",[t._v("X")]),e("mo",[t._v(")")]),e("mo",[t._v("=")]),e("mi",[t._v("X")]),e("msub",[e("mi",[t._v("W")]),e("mrow",[e("mi",[t._v("Q")])],1)],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("Q = \\text{Linear}(X) = X W_{Q}\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1.036108em","vertical-align":"-0.286108em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord mathit"},[t._v("Q")]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"text mord displaystyle textstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[t._v("L")]),e("span",{staticClass:"mord mathrm"},[t._v("i")]),e("span",{staticClass:"mord mathrm"},[t._v("n")]),e("span",{staticClass:"mord mathrm"},[t._v("e")]),e("span",{staticClass:"mord mathrm"},[t._v("a")]),e("span",{staticClass:"mord mathrm"},[t._v("r")])]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"mclose"},[t._v(")")]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("Q")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])])])]),t._v("\nK = \\text{Linear}(X) = XW_{K} \\tag{3}\n\n"),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("V")]),e("mo",[t._v("=")]),e("mtext",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("L")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("n")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("a")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("r")])],1),e("mo",[t._v("(")]),e("mi",[t._v("X")]),e("mo",[t._v(")")]),e("mo",[t._v("=")]),e("mi",[t._v("X")]),e("msub",[e("mi",[t._v("W")]),e("mrow",[e("mi",[t._v("V")])],1)],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("V = \\text{Linear}(X) = XW_{V}\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.22222em"}},[t._v("V")]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"text mord displaystyle textstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[t._v("L")]),e("span",{staticClass:"mord mathrm"},[t._v("i")]),e("span",{staticClass:"mord mathrm"},[t._v("n")]),e("span",{staticClass:"mord mathrm"},[t._v("e")]),e("span",{staticClass:"mord mathrm"},[t._v("a")]),e("span",{staticClass:"mord mathrm"},[t._v("r")])]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"mclose"},[t._v(")")]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.22222em"}},[t._v("V")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])])])]),t._v("\nX_{\\text{attention}} = \\text{SelfAttention}(Q, K, V) \\tag{4}\n\n"),e("ol",{attrs:{start:"3"}},[e("li",[e("strong",[t._v("层归一化、残差连接")])])]),t._v("\nX_{\\text{attention}} = \\text{LayerNorm}(X_{\\text{attention}}) \\tag{5}\n\nX_{\\text{attention}} = X + X_{\\text{attention}} \\tag{6}\n\n"),e("ol",{attrs:{start:"4"}},[e("li",[e("strong",[t._v("前馈网络")])])]),t._v("\nX_{\\text{hidden}} = \\text{Linear}(\\text{Activate}(\\text{Linear}(X_{\\text{attention}}))) \\tag{7}\n\n"),e("ol",{attrs:{start:"5"}},[e("li",[e("strong",[t._v("层归一化、残差连接")])])]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("X")]),e("mrow",[e("mtext",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("h")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("d")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("d")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("n")])],1)],1)],1),e("mo",[t._v("=")]),e("mtext",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("L")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("a")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("y")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("r")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("N")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("o")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("r")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("m")])],1),e("mo",[t._v("(")]),e("msub",[e("mi",[t._v("X")]),e("mrow",[e("mtext",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("h")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("d")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("d")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("n")])],1)],1)],1),e("mo",[t._v(")")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X_{\\text{hidden}} = \\text{LayerNorm}(X_{\\text{hidden}})\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.07847em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"text mord scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("h")]),e("span",{staticClass:"mord mathrm"},[t._v("i")]),e("span",{staticClass:"mord mathrm"},[t._v("d")]),e("span",{staticClass:"mord mathrm"},[t._v("d")]),e("span",{staticClass:"mord mathrm"},[t._v("e")]),e("span",{staticClass:"mord mathrm"},[t._v("n")])])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"text mord displaystyle textstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[t._v("L")]),e("span",{staticClass:"mord mathrm"},[t._v("a")]),e("span",{staticClass:"mord mathrm",staticStyle:{"margin-right":"0.01389em"}},[t._v("y")]),e("span",{staticClass:"mord mathrm"},[t._v("e")]),e("span",{staticClass:"mord mathrm"},[t._v("r")]),e("span",{staticClass:"mord mathrm"},[t._v("N")]),e("span",{staticClass:"mord mathrm"},[t._v("o")]),e("span",{staticClass:"mord mathrm"},[t._v("r")]),e("span",{staticClass:"mord mathrm"},[t._v("m")])]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.07847em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"text mord scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("h")]),e("span",{staticClass:"mord mathrm"},[t._v("i")]),e("span",{staticClass:"mord mathrm"},[t._v("d")]),e("span",{staticClass:"mord mathrm"},[t._v("d")]),e("span",{staticClass:"mord mathrm"},[t._v("e")]),e("span",{staticClass:"mord mathrm"},[t._v("n")])])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mclose"},[t._v(")")])])])])])]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("X")]),e("mrow",[e("mtext",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("h")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("d")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("d")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("n")])],1)],1)],1),e("mo",[t._v("=")]),e("msub",[e("mi",[t._v("X")]),e("mrow",[e("mtext",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("a")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("t")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("t")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("n")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("t")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("o")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("n")])],1)],1)],1),e("mo",[t._v("+")]),e("msub",[e("mi",[t._v("X")]),e("mrow",[e("mtext",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("h")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("i")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("d")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("d")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("e")]),e("mi",{attrs:{mathvariant:"normal"}},[t._v("n")])],1)],1)],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X_{\\text{hidden}} = X_{\\text{attention}} + X_{\\text{hidden}}\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.83333em","vertical-align":"-0.15em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.07847em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"text mord scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("h")]),e("span",{staticClass:"mord mathrm"},[t._v("i")]),e("span",{staticClass:"mord mathrm"},[t._v("d")]),e("span",{staticClass:"mord mathrm"},[t._v("d")]),e("span",{staticClass:"mord mathrm"},[t._v("e")]),e("span",{staticClass:"mord mathrm"},[t._v("n")])])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.07847em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"text mord scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("a")]),e("span",{staticClass:"mord mathrm"},[t._v("t")]),e("span",{staticClass:"mord mathrm"},[t._v("t")]),e("span",{staticClass:"mord mathrm"},[t._v("e")]),e("span",{staticClass:"mord mathrm"},[t._v("n")]),e("span",{staticClass:"mord mathrm"},[t._v("t")]),e("span",{staticClass:"mord mathrm"},[t._v("i")]),e("span",{staticClass:"mord mathrm"},[t._v("o")]),e("span",{staticClass:"mord mathrm"},[t._v("n")])])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mbin"},[t._v("+")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.07847em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"text mord scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("h")]),e("span",{staticClass:"mord mathrm"},[t._v("i")]),e("span",{staticClass:"mord mathrm"},[t._v("d")]),e("span",{staticClass:"mord mathrm"},[t._v("d")]),e("span",{staticClass:"mord mathrm"},[t._v("e")]),e("span",{staticClass:"mord mathrm"},[t._v("n")])])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])])])]),t._v("\nX_{\\text{hidden}} \\in \\mathbb{R}^{\\text{batch_size} \\times \\text{seq_len} \\times \\text{embedding_dim}}\n\n"),e("p",[t._v("Transformer编码器由"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("N")]),e("mo",[t._v("=")]),e("mn",[t._v("6")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("N = 6")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.10903em"}},[t._v("N")]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord mathrm"},[t._v("6")])])])]),t._v("个编码器基本单元组成。")])])}),[],!1,null,null,null);a.default=i.exports}}]);