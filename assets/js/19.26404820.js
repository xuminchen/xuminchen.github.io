(window.webpackJsonp=window.webpackJsonp||[]).push([[19],{384:function(t,a,s){"use strict";s.r(a);var n=s(42),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"task3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#task3"}},[t._v("#")]),t._v(" Task3")]),t._v(" "),s("h2",{attrs:{id:"introduction"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),s("p",[t._v("特征工程是在机器学习等领域中至关重要的一步，数据处理的好坏、特征筛选、编码等都会影响到后续模型的性能。因此特征工程是非常重要的一个环节。")]),t._v(" "),s("h2",{attrs:{id:"step"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step"}},[t._v("#")]),t._v(" Step")]),t._v(" "),s("ul",[s("li",[t._v("数据预处理\n"),s("ul",[s("li",[t._v("缺失值的填充")]),t._v(" "),s("li",[t._v("时间格式处理")]),t._v(" "),s("li",[t._v("对象类型特征转换到数值")])])]),t._v(" "),s("li",[t._v("异常值处理\n"),s("ul",[s("li",[t._v("基于3segama原则")]),t._v(" "),s("li",[t._v("基于箱型图")])])]),t._v(" "),s("li",[t._v("数据分箱\n"),s("ul",[s("li",[t._v("固定宽度分箱")]),t._v(" "),s("li",[t._v("分位数分箱\n"),s("ul",[s("li",[t._v("离散数值型数据分箱")]),t._v(" "),s("li",[t._v("连续数值型数据分箱")])])]),t._v(" "),s("li",[t._v("卡方分箱（选做作业）")])])]),t._v(" "),s("li",[t._v("特征交互\n"),s("ul",[s("li",[t._v("特征和特征之间组合")]),t._v(" "),s("li",[t._v("特征和特征之间衍生")]),t._v(" "),s("li",[t._v("其他特征衍生的尝试（选做作业）")])])]),t._v(" "),s("li",[t._v("特征编码\n"),s("ul",[s("li",[t._v("one-hot编码")]),t._v(" "),s("li",[t._v("label-encode编码")])])]),t._v(" "),s("li",[t._v("特征选择\n"),s("ul",[s("li",[t._v("1 Filter")]),t._v(" "),s("li",[t._v("2 Wrapper （RFE）")]),t._v(" "),s("li",[t._v("3 Embedded")])])])]),t._v(" "),s("h2",{attrs:{id:"_3-3-代码示例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-代码示例"}},[t._v("#")]),t._v(" 3.3 代码示例")]),t._v(" "),s("h3",{attrs:{id:"_3-3-1-导入包并读取数据"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-1-导入包并读取数据"}},[t._v("#")]),t._v(" 3.3.1 导入包并读取数据")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" seaborn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sns\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" datetime\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tqdm "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tqdm\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preprocessing "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LabelEncoder\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SelectKBest\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" chi2\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preprocessing "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MinMaxScaler\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" xgboost "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" xgb\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" lightgbm "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" lgb\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" catboost "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" CatBoostRegressor\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" warnings\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" StratifiedKFold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" KFold\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("metrics "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" accuracy_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" f1_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" roc_auc_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" log_loss\nwarnings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filterwarnings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ignore'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("data_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../train.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_test_a "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../testA.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"_3-3-2特征预处理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-2特征预处理"}},[t._v("#")]),t._v(" 3.3.2特征预处理")]),t._v(" "),s("ul",[s("li",[t._v("数据EDA部分我们已经对数据的大概和某些特征分布有了了解，数据预处理部分一般我们要处理一些EDA阶段分析出来的问题，这里介绍了数据缺失值的填充，时间格式特征的转化处理，某些对象类别特征的处理。")])]),t._v(" "),s("p",[t._v("首先我们查找出数据中的对象特征和数值特征")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("numerical_fea "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select_dtypes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("exclude"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'object'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("columns"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncategory_fea "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("filter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("columns"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlabel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'isDefault'")]),t._v("\nnumerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("remove"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("在比赛中数据预处理是必不可少的一部分，对于缺失值的填充往往会影响比赛的结果，在比赛中不妨尝试多种填充然后比较结果选择结果最优的一种；\n比赛数据相比真实场景的数据相对要“干净”一些，但是还是会有一定的“脏”数据存在，清洗一些异常值往往会获得意想不到的效果。")]),t._v(" "),s("h4",{attrs:{id:"缺失值填充"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#缺失值填充"}},[t._v("#")]),t._v(" 缺失值填充")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("把所有缺失值替换为指定的值0")]),t._v(" "),s("p",[t._v("data_train = data_train.fillna(0)")])]),t._v(" "),s("li",[s("p",[t._v("向用缺失值上面的值替换缺失值")]),t._v(" "),s("p",[t._v("data_train = data_train.fillna(axis=0,method='ffill')")])]),t._v(" "),s("li",[s("p",[t._v("纵向用缺失值下面的值替换缺失值,且设置最多只填充两个连续的缺失值")]),t._v(" "),s("p",[t._v("data_train = data_train.fillna(axis=0,method='bfill',limit=2)")])])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#查看缺失值情况")]),t._v("\ndata_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isnull"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("id                        0\nloanAmnt                  0\nterm                      0\ninterestRate              0\ninstallment               0\ngrade                     0\nsubGrade                  0\nemploymentTitle           1\nemploymentLength      46799\nhomeOwnership             0\nannualIncome              0\nverificationStatus        0\nissueDate                 0\nisDefault                 0\npurpose                   0\npostCode                  1\nregionCode                0\ndti                     239\ndelinquency_2years        0\nficoRangeLow              0\nficoRangeHigh             0\nopenAcc                   0\npubRec                    0\npubRecBankruptcies      405\nrevolBal                  0\nrevolUtil               531\ntotalAcc                  0\ninitialListStatus         0\napplicationType           0\nearliesCreditLine         0\ntitle                     1\npolicyCode                0\nn0                    40270\nn1                    40270\nn2                    40270\nn2.1                  40270\nn4                    33239\nn5                    40270\nn6                    40270\nn7                    40270\nn8                    40271\nn9                    40270\nn10                   33239\nn11                   69752\nn12                   40270\nn13                   40270\nn14                   40270\ndtype: int64\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#按照平均数填充数值型特征")]),t._v("\ndata_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("median"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("median"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#按照众数填充类别型特征")]),t._v("\ndata_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("category_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("category_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("category_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("category_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("category_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("category_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isnull"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("id                        0\nloanAmnt                  0\nterm                      0\ninterestRate              0\ninstallment               0\ngrade                     0\nsubGrade                  0\nemploymentTitle           0\nemploymentLength      46799\nhomeOwnership             0\nannualIncome              0\nverificationStatus        0\nissueDate                 0\nisDefault                 0\npurpose                   0\npostCode                  0\nregionCode                0\ndti                       0\ndelinquency_2years        0\nficoRangeLow              0\nficoRangeHigh             0\nopenAcc                   0\npubRec                    0\npubRecBankruptcies        0\nrevolBal                  0\nrevolUtil                 0\ntotalAcc                  0\ninitialListStatus         0\napplicationType           0\nearliesCreditLine         0\ntitle                     0\npolicyCode                0\nn0                        0\nn1                        0\nn2                        0\nn2.1                      0\nn4                        0\nn5                        0\nn6                        0\nn7                        0\nn8                        0\nn9                        0\nn10                       0\nn11                       0\nn12                       0\nn13                       0\nn14                       0\ndtype: int64\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#查看类别特征")]),t._v("\ncategory_fea\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("['grade', 'subGrade', 'employmentLength', 'issueDate', 'earliesCreditLine']\n")])])]),s("ul",[s("li",[t._v("category_fea：对象型类别特征需要进行预处理，其中['issueDate']为时间格式特征。")])]),t._v(" "),s("h4",{attrs:{id:"时间格式处理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#时间格式处理"}},[t._v("#")]),t._v(" 时间格式处理")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#转化成时间格式")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'issueDate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_datetime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'issueDate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'%Y-%m-%d'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    startdate "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" datetime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datetime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strptime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2007-06-01'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'%Y-%m-%d'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#构造时间特征")]),t._v("\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'issueDateDT'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'issueDate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("startdate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("days\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'employmentLength'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dropna"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sort_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("1 year        52489\n10+ years    262753\n2 years       72358\n3 years       64152\n4 years       47985\n5 years       50102\n6 years       37254\n7 years       35407\n8 years       36192\n9 years       30272\n< 1 year      64237\nNaN           46799\nName: employmentLength, dtype: int64\n")])])]),s("h4",{attrs:{id:"对象类型特征转换到数值"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#对象类型特征转换到数值"}},[t._v("#")]),t._v(" 对象类型特征转换到数值")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("employmentLength_to_int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isnull"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" s\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("int8"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'employmentLength'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to_replace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'10+ years'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'10 years'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'employmentLength'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'< 1 year'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'0 years'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'employmentLength'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'employmentLength'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("employmentLength_to_int"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'employmentLength'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dropna"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sort_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("0.0     15989\n1.0     13182\n2.0     18207\n3.0     16011\n4.0     11833\n5.0     12543\n6.0      9328\n7.0      8823\n8.0      8976\n9.0      7594\n10.0    65772\nNaN     11742\nName: employmentLength, dtype: int64\n")])])]),s("ul",[s("li",[t._v("对earliesCreditLine进行预处理")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'earliesCreditLine'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("519915    Sep-2002\n564368    Dec-1996\n768209    May-2004\n453092    Nov-1995\n763866    Sep-2000\nName: earliesCreditLine, dtype: object\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'earliesCreditLine'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'earliesCreditLine'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"类别特征处理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#类别特征处理"}},[t._v("#")]),t._v(" 类别特征处理")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 部分类别特征")]),t._v("\ncate_features "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'subGrade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'employmentTitle'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'homeOwnership'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'verificationStatus'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'purpose'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'postCode'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'regionCode'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \\\n                 "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'applicationType'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'initialListStatus'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'policyCode'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" f "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" cate_features"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'类型数：'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nunique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("grade 类型数： 7\nsubGrade 类型数： 35\nemploymentTitle 类型数： 79282\nhomeOwnership 类型数： 6\nverificationStatus 类型数： 3\npurpose 类型数： 14\npostCode 类型数： 889\nregionCode 类型数： 51\napplicationType 类型数： 2\ninitialListStatus 类型数： 2\ntitle 类型数： 12058\npolicyCode 类型数： 1\n")])])]),s("p",[t._v("像等级这种类别特征，是有优先级的可以labelencode或者自映射")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'A'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'C'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'D'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'E'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'F'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'G'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类型数在2之上，又不是高维稀疏的,且纯分类特征")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_dummies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" columns"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'subGrade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'homeOwnership'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'verificationStatus'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'purpose'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'regionCode'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" drop_first"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"_3-3-3-异常值处理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-3-异常值处理"}},[t._v("#")]),t._v(" 3.3.3  异常值处理")]),t._v(" "),s("ul",[s("li",[t._v("当你发现异常值后，一定要先分清是什么原因导致的异常值，然后再考虑如何处理。首先，如果这一异常值并不代表一种规律性的，而是极其偶然的现象，或者说你并不想研究这种偶然的现象，这时可以将其删除。其次，如果异常值存在且代表了一种真实存在的现象，那就不能随便删除。在现有的欺诈场景中很多时候欺诈数据本身相对于正常数据勒说就是异常的，我们要把这些异常点纳入，重新拟合模型，研究其规律。能用监督的用监督模型，不能用的还可以考虑用异常检测的算法来做。")]),t._v(" "),s("li",[t._v("注意test的数据不能删。")])]),t._v(" "),s("h4",{attrs:{id:"检测异常的方法一-均方差"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#检测异常的方法一-均方差"}},[t._v("#")]),t._v(" 检测异常的方法一：均方差")]),t._v(" "),s("p",[t._v("在统计学中，如果一个数据分布近似正态，那么大约 68% 的数据值会在均值的一个标准差范围内，大约 95% 会在两个标准差范围内，大约 99.7% 会在三个标准差范围内。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("find_outliers_by_3segama")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data_std "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("std"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    data_mean "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    outliers_cut_off "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_std "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\n    lower_rule "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_mean "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" outliers_cut_off\n    upper_rule "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_mean "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" outliers_cut_off\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_outliers'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'异常值'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" upper_rule "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" lower_rule "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'正常值'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" data\n")])])]),s("ul",[s("li",[t._v("得到特征的异常值后可以进一步分析变量异常值和目标变量的关系")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("data_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("copy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" fea "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" find_outliers_by_3segama"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_outliers'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_outliers'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'isDefault'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'*'")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("正常值    800000\nName: id_outliers, dtype: int64\nid_outliers\n正常值    159610\nName: isDefault, dtype: int64\n**********\n正常值    800000\nName: loanAmnt_outliers, dtype: int64\nloanAmnt_outliers\n正常值    159610\nName: isDefault, dtype: int64\n**********\n正常值    800000\nName: term_outliers, dtype: int64\nterm_outliers\n正常值    159610\nName: isDefault, dtype: int64\n**********\n正常值    794259\n异常值      5741\nName: interestRate_outliers, dtype: int64\ninterestRate_outliers\n异常值      2916\n正常值    156694\nName: isDefault, dtype: int64\n**********\n正常值    792046\n异常值      7954\nName: installment_outliers, dtype: int64\ninstallment_outliers\n异常值      2152\n正常值    157458\nName: isDefault, dtype: int64\n**********\n正常值    800000\nName: employmentTitle_outliers, dtype: int64\nemploymentTitle_outliers\n正常值    159610\nName: isDefault, dtype: int64\n**********\n正常值    799701\n异常值       299\nName: homeOwnership_outliers, dtype: int64\nhomeOwnership_outliers\n异常值        62\n正常值    159548\nName: isDefault, dtype: int64\n**********\n正常值    793973\n异常值      6027\nName: annualIncome_outliers, dtype: int64\nannualIncome_outliers\n异常值       756\n正常值    158854\nName: isDefault, dtype: int64\n**********\n正常值    800000\nName: verificationStatus_outliers, dtype: int64\nverificationStatus_outliers\n正常值    159610\nName: isDefault, dtype: int64\n**********\n正常值    783003\n异常值     16997\nName: purpose_outliers, dtype: int64\npurpose_outliers\n异常值      3635\n正常值    155975\nName: isDefault, dtype: int64\n**********\n正常值    798931\n异常值      1069\nName: postCode_outliers, dtype: int64\npostCode_outliers\n异常值       221\n正常值    159389\nName: isDefault, dtype: int64\n**********\n正常值    799994\n异常值         6\nName: regionCode_outliers, dtype: int64\nregionCode_outliers\n异常值         1\n正常值    159609\nName: isDefault, dtype: int64\n**********\n正常值    798440\n异常值      1560\nName: dti_outliers, dtype: int64\ndti_outliers\n异常值       466\n正常值    159144\nName: isDefault, dtype: int64\n**********\n正常值    778245\n异常值     21755\nName: delinquency_2years_outliers, dtype: int64\ndelinquency_2years_outliers\n异常值      5089\n正常值    154521\nName: isDefault, dtype: int64\n**********\n正常值    788261\n异常值     11739\nName: ficoRangeLow_outliers, dtype: int64\nficoRangeLow_outliers\n异常值       778\n正常值    158832\nName: isDefault, dtype: int64\n**********\n正常值    788261\n异常值     11739\nName: ficoRangeHigh_outliers, dtype: int64\nficoRangeHigh_outliers\n异常值       778\n正常值    158832\nName: isDefault, dtype: int64\n**********\n正常值    790889\n异常值      9111\nName: openAcc_outliers, dtype: int64\nopenAcc_outliers\n异常值      2195\n正常值    157415\nName: isDefault, dtype: int64\n**********\n正常值    792471\n异常值      7529\nName: pubRec_outliers, dtype: int64\npubRec_outliers\n异常值      1701\n正常值    157909\nName: isDefault, dtype: int64\n**********\n正常值    794120\n异常值      5880\nName: pubRecBankruptcies_outliers, dtype: int64\npubRecBankruptcies_outliers\n异常值      1423\n正常值    158187\nName: isDefault, dtype: int64\n**********\n正常值    790001\n异常值      9999\nName: revolBal_outliers, dtype: int64\nrevolBal_outliers\n异常值      1359\n正常值    158251\nName: isDefault, dtype: int64\n**********\n正常值    799948\n异常值        52\nName: revolUtil_outliers, dtype: int64\nrevolUtil_outliers\n异常值        23\n正常值    159587\nName: isDefault, dtype: int64\n**********\n正常值    791663\n异常值      8337\nName: totalAcc_outliers, dtype: int64\ntotalAcc_outliers\n异常值      1668\n正常值    157942\nName: isDefault, dtype: int64\n**********\n正常值    800000\nName: initialListStatus_outliers, dtype: int64\ninitialListStatus_outliers\n正常值    159610\nName: isDefault, dtype: int64\n**********\n正常值    784586\n异常值     15414\nName: applicationType_outliers, dtype: int64\napplicationType_outliers\n异常值      3875\n正常值    155735\nName: isDefault, dtype: int64\n**********\n正常值    775134\n异常值     24866\nName: title_outliers, dtype: int64\ntitle_outliers\n异常值      3900\n正常值    155710\nName: isDefault, dtype: int64\n**********\n正常值    800000\nName: policyCode_outliers, dtype: int64\npolicyCode_outliers\n正常值    159610\nName: isDefault, dtype: int64\n**********\n正常值    782773\n异常值     17227\nName: n0_outliers, dtype: int64\nn0_outliers\n异常值      3485\n正常值    156125\nName: isDefault, dtype: int64\n**********\n正常值    790500\n异常值      9500\nName: n1_outliers, dtype: int64\nn1_outliers\n异常值      2491\n正常值    157119\nName: isDefault, dtype: int64\n**********\n正常值    789067\n异常值     10933\nName: n2_outliers, dtype: int64\nn2_outliers\n异常值      3205\n正常值    156405\nName: isDefault, dtype: int64\n**********\n正常值    789067\n异常值     10933\nName: n2.1_outliers, dtype: int64\nn2.1_outliers\n异常值      3205\n正常值    156405\nName: isDefault, dtype: int64\n**********\n正常值    788660\n异常值     11340\nName: n4_outliers, dtype: int64\nn4_outliers\n异常值      2476\n正常值    157134\nName: isDefault, dtype: int64\n**********\n正常值    790355\n异常值      9645\nName: n5_outliers, dtype: int64\nn5_outliers\n异常值      1858\n正常值    157752\nName: isDefault, dtype: int64\n**********\n正常值    786006\n异常值     13994\nName: n6_outliers, dtype: int64\nn6_outliers\n异常值      3182\n正常值    156428\nName: isDefault, dtype: int64\n**********\n正常值    788430\n异常值     11570\nName: n7_outliers, dtype: int64\nn7_outliers\n异常值      2746\n正常值    156864\nName: isDefault, dtype: int64\n**********\n正常值    789625\n异常值     10375\nName: n8_outliers, dtype: int64\nn8_outliers\n异常值      2131\n正常值    157479\nName: isDefault, dtype: int64\n**********\n正常值    786384\n异常值     13616\nName: n9_outliers, dtype: int64\nn9_outliers\n异常值      3953\n正常值    155657\nName: isDefault, dtype: int64\n**********\n正常值    788979\n异常值     11021\nName: n10_outliers, dtype: int64\nn10_outliers\n异常值      2639\n正常值    156971\nName: isDefault, dtype: int64\n**********\n正常值    799434\n异常值       566\nName: n11_outliers, dtype: int64\nn11_outliers\n异常值       112\n正常值    159498\nName: isDefault, dtype: int64\n**********\n正常值    797585\n异常值      2415\nName: n12_outliers, dtype: int64\nn12_outliers\n异常值       545\n正常值    159065\nName: isDefault, dtype: int64\n**********\n正常值    788907\n异常值     11093\nName: n13_outliers, dtype: int64\nn13_outliers\n异常值      2482\n正常值    157128\nName: isDefault, dtype: int64\n**********\n正常值    788884\n异常值     11116\nName: n14_outliers, dtype: int64\nn14_outliers\n异常值      3364\n正常值    156246\nName: isDefault, dtype: int64\n**********\n")])])]),s("ul",[s("li",[t._v("例如可以看到异常值在两个变量上的分布几乎复合整体的分布，如果异常值都属于为1的用户数据里面代表什么呢？")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#删除异常值")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" fea "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_outliers'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'正常值'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    data_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reset_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("drop"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n")])])]),s("h4",{attrs:{id:"检测异常的方法二-箱型图"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#检测异常的方法二-箱型图"}},[t._v("#")]),t._v(" 检测异常的方法二：箱型图")]),t._v(" "),s("ul",[s("li",[t._v("总结一句话：四分位数会将数据分为三个点和四个区间，IQR = Q3 -Q1，下触须=Q1 − 1.5x IQR，上触须=Q3 + 1.5x IQR；")])]),t._v(" "),s("h3",{attrs:{id:"_3-3-4-数据分桶"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-4-数据分桶"}},[t._v("#")]),t._v(" 3.3.4 数据分桶")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("特征分箱的目的：")]),t._v(" "),s("ul",[s("li",[t._v("从模型效果上来看，特征分箱主要是为了降低变量的复杂性，减少变量噪音对模型的影响，提高自变量和因变量的相关度。从而使模型更加稳定。")])])]),t._v(" "),s("li",[s("p",[t._v("数据分桶的对象：")]),t._v(" "),s("ul",[s("li",[t._v("将连续变量离散化")]),t._v(" "),s("li",[t._v("将多状态的离散变量合并成少状态")])])]),t._v(" "),s("li",[s("p",[t._v("分箱的原因：")]),t._v(" "),s("ul",[s("li",[t._v("数据的特征内的值跨度可能比较大，对有监督和无监督中如k-均值聚类它使用欧氏距离作为相似度函数来测量数据点之间的相似度。都会造成大吃小的影响，其中一种解决方法是对计数值进行区间量化即数据分桶也叫做数据分箱，然后使用量化后的结果。")])])]),t._v(" "),s("li",[s("p",[t._v("分箱的优点：")]),t._v(" "),s("ul",[s("li",[t._v("处理缺失值：当数据源可能存在缺失值，此时可以把null单独作为一个分箱。")]),t._v(" "),s("li",[t._v("处理异常值：当数据中存在离群点时，可以把其通过分箱离散化处理，从而提高变量的鲁棒性（抗干扰能力）。例如，age若出现200这种异常值，可分入“age > 60”这个分箱里，排除影响。")]),t._v(" "),s("li",[t._v("业务解释性：我们习惯于线性判断变量的作用，当x越来越大，y就越来越大。但实际x与y之间经常存在着非线性关系，此时可经过WOE变换。")])])]),t._v(" "),s("li",[s("p",[t._v("特别要注意一下分箱的基本原则：")]),t._v(" "),s("ul",[s("li",[t._v("（1）最小分箱占比不低于5%")]),t._v(" "),s("li",[t._v("（2）箱内不能全部是好客户")]),t._v(" "),s("li",[t._v("（3）连续箱单调")])])])]),t._v(" "),s("ol",[s("li",[t._v("固定宽度分箱")])]),t._v(" "),s("p",[t._v("当数值横跨多个数量级时，最好按照 10 的幂（或任何常数的幂）来进行分组：0~9、10~99、100~999、1000~9999，等等。固定宽度分箱非常容易计算，但如果计数值中有比较大的缺口，就会产生很多没有任何数据的空箱子。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 通过除法映射到间隔均匀的分箱中，每个分箱的取值范围都是loanAmnt/1000")]),t._v("\ndata"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loanAmnt_bin1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("floor_divide"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loanAmnt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("## 通过对数函数映射到指数宽度分箱")]),t._v("\ndata"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loanAmnt_bin2'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("floor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log10"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loanAmnt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ol",{attrs:{start:"2"}},[s("li",[t._v("分位数分箱")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loanAmnt_bin3'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("qcut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loanAmnt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ol",{attrs:{start:"3"}},[s("li",[t._v("卡方分箱及其他分箱方法的尝试")])]),t._v(" "),s("ul",[s("li",[t._v("这一部分属于进阶部分，学有余力的同学可以自行搜索尝试。")])]),t._v(" "),s("h3",{attrs:{id:"_3-3-5-特征交互"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-5-特征交互"}},[t._v("#")]),t._v(" 3.3.5 特征交互")]),t._v(" "),s("ul",[s("li",[t._v("交互特征的构造非常简单，使用起来却代价不菲。如果线性模型中包含有交互特征对，那它的训练时间和评分时间就会从 O(n) 增加到 O(n2)，其中 n 是单一特征的数量。")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" col "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'subGrade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" \n    temp_dict "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'isDefault'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("agg"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reset_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("columns"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" col "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_target_mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    temp_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values\n    temp_dict "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_target_mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_target_mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("temp_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_target_mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("temp_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 其他衍生变量 mean 和 std")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" df "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" item "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n0'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n2'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n2.1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n4'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n5'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n6'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n7'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n8'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n9'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n10'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n11'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n12'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n13'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n14'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade_to_mean_'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mean'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade_to_std_'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'grade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'std'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("这里给出一些特征交互的思路，但特征和特征间的交互衍生出新的特征还远远不止于此，抛砖引玉，希望大家多多探索。请学习者尝试其他的特征交互方法。")]),t._v(" "),s("h3",{attrs:{id:"_3-3-6-特征编码"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-6-特征编码"}},[t._v("#")]),t._v(" 3.3.6 特征编码")]),t._v(" "),s("h4",{attrs:{id:"labelencode-直接放入树模型中"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#labelencode-直接放入树模型中"}},[t._v("#")]),t._v(" labelEncode 直接放入树模型中")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#label-encode:subGrade,postCode,title")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 高维类别特征需要进行转换")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" col "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" tqdm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'employmentTitle'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'postCode'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'subGrade'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    le "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LabelEncoder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    le"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" le"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" le"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Label Encoding 完成'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("100%|██████████| 4/4 [00:08<00:00,  2.04s/it]\n\nLabel Encoding 完成\n")])])]),s("p",[t._v("​"),s("br"),t._v("\n​")]),t._v(" "),s("h4",{attrs:{id:"逻辑回归等模型要单独增加的特征工程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#逻辑回归等模型要单独增加的特征工程"}},[t._v("#")]),t._v(" 逻辑回归等模型要单独增加的特征工程")]),t._v(" "),s("ul",[s("li",[t._v("对特征做归一化，去除相关性高的特征")]),t._v(" "),s("li",[t._v("归一化目的是让训练过程更好更快的收敛，避免特征大吃小的问题")]),t._v(" "),s("li",[t._v("去除相关性是增加模型的可解释性，加快预测过程。")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 举例归一化过程")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#伪代码")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" fea "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("要归一化的特征列表"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("：\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("min")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("min")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"_3-3-7-特征选择"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-7-特征选择"}},[t._v("#")]),t._v(" 3.3.7 特征选择")]),t._v(" "),s("ul",[s("li",[t._v("特征选择技术可以精简掉无用的特征，以降低最终模型的复杂性，它的最终目的是得到一个简约模型，在不降低预测准确率或对预测准确率影响不大的情况下提高计算速度。特征选择不是为了减少训练时间（实际上，一些技术会增加总体训练时间），而是为了减少模型评分时间。")])]),t._v(" "),s("p",[t._v("特征选择的方法：")]),t._v(" "),s("ul",[s("li",[t._v("1 Filter\n"),s("ul",[s("li",[t._v("方差选择法")]),t._v(" "),s("li",[t._v("相关系数法（pearson 相关系数）")]),t._v(" "),s("li",[t._v("卡方检验")]),t._v(" "),s("li",[t._v("互信息法")])])]),t._v(" "),s("li",[t._v("2 Wrapper （RFE）\n"),s("ul",[s("li",[t._v("递归特征消除法")])])]),t._v(" "),s("li",[t._v("3 Embedded\n"),s("ul",[s("li",[t._v("基于惩罚项的特征选择法")]),t._v(" "),s("li",[t._v("基于树模型的特征选择")])])])]),t._v(" "),s("h4",{attrs:{id:"filter"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#filter"}},[t._v("#")]),t._v(" Filter")]),t._v(" "),s("ul",[s("li",[t._v("基于特征间的关系进行筛选")])]),t._v(" "),s("p",[t._v("方差选择法")]),t._v(" "),s("ul",[s("li",[t._v("方差选择法中，先要计算各个特征的方差，然后根据设定的阈值，选择方差大于阈值的特征")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VarianceThreshold\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#其中参数threshold为方差的阈值")]),t._v("\nVarianceThreshold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("threshold"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("target_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("相关系数法")]),t._v(" "),s("ul",[s("li",[t._v("Pearson 相关系数\n皮尔森相关系数是一种最简单的，可以帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性。\n结果的取值区间为 [-1，1] ， -1 表示完全的负相关， +1表示完全的正相关，0 表示没有线性相关。")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SelectKBest\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" scipy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pearsonr\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#选择K个最好的特征，返回选择特征后的数据")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#参数k为选择的特征个数")]),t._v("\n\nSelectKBest"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("target_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("卡方检验")]),t._v(" "),s("ul",[s("li",[t._v("经典的卡方检验是用于检验自变量对因变量的相关性。 假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距。 其统计量如下： χ2=∑(A−T)2T，其中A为实际值，T为理论值")]),t._v(" "),s("li",[t._v("(注：卡方只能运用在正定矩阵上，否则会报错Input X must be non-negative)")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SelectKBest\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" chi2\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#参数k为选择的特征个数")]),t._v("\n\nSelectKBest"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("chi2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("target_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("互信息法")]),t._v(" "),s("ul",[s("li",[t._v("经典的互信息也是评价自变量对因变量的相关性的。 在feature_selection库的SelectKBest类结合最大信息系数法可以用于选择特征，相关代码如下：")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SelectKBest\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" minepy "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MINE\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#返回一个二元组，二元组的第2项设置成固定的P值0.5")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("mic")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    m "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MINE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mic"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#参数k为选择的特征个数")]),t._v("\nSelectKBest"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" X"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" array"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("mic"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" X"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("target_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"wrapper-recursive-feature-elimination-rfe"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#wrapper-recursive-feature-elimination-rfe"}},[t._v("#")]),t._v(" Wrapper （Recursive feature elimination，RFE）")]),t._v(" "),s("ul",[s("li",[t._v("递归特征消除法 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 在feature_selection库的RFE类可以用于选择特征，相关代码如下（以逻辑回归为例）：")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" RFE\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear_model "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LogisticRegression\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#递归特征消除法，返回特征选择后的数据")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#参数estimator为基模型")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#参数n_features_to_select为选择的特征个数")]),t._v("\n\nRFE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("estimator"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("LogisticRegression"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_features_to_select"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("target_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"embedded"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#embedded"}},[t._v("#")]),t._v(" Embedded")]),t._v(" "),s("ul",[s("li",[t._v("基于惩罚项的特征选择法 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。 在feature_selection库的SelectFromModel类结合逻辑回归模型可以用于选择特征，相关代码如下：")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SelectFromModel\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear_model "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LogisticRegression\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#带L1惩罚项的逻辑回归作为基模型的特征选择")]),t._v("\n\nSelectFromModel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("LogisticRegression"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("penalty"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"l1"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" C"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("target_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ul",[s("li",[t._v("基于树模型的特征选择 树模型中GBDT也可用来作为基模型进行特征选择。 在feature_selection库的SelectFromModel类结合GBDT模型可以用于选择特征，相关代码如下：")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SelectFromModel\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ensemble "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" GradientBoostingClassifier\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#GBDT作为基模型的特征选择")]),t._v("\nSelectFromModel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("GradientBoostingClassifier"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("target_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("本数据集中我们删除非入模特征后，并对缺失值填充，然后用计算协方差的方式看一下特征间相关性，然后进行模型训练")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 删除不需要的数据")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'issueDate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'id'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("inplace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"纵向用缺失值上面的值替换缺失值"')]),t._v("\ndata_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("method"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ffill'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("x_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'isDefault'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'id'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#计算协方差")]),t._v("\ndata_corr "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corrwith"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isDefault"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#计算相关性")]),t._v("\nresult "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("columns"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'features'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'corr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nresult"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'features'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_corr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index\nresult"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'corr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_corr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 当然也可以直接看图")]),t._v("\ndata_numeric "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("numerical_fea"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ncorrelation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_numeric"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nf "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ax "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" plt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("subplots"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("figsize "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("title"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Correlation of Numeric Features with Price'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsns"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("heatmap"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("correlation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("square "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  vmax"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("<matplotlib.axes._subplots.AxesSubplot at 0x12d88ad10>\n")])])]),s("p",[s("img",{attrs:{src:"https://img-blog.csdnimg.cn/2020090509380436.png",alt:"output_81_1.png"}})]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("features "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("f "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" f "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("columns "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" f "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'id'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'issueDate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'isDefault'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_outliers'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nx_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("features"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nx_test "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_test_a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("features"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ny_train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'isDefault'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("cv_model")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("clf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clf_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    folds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\n    seed "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2020")]),t._v("\n    kf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" KFold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("folds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" random_state"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("seed"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    test "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    cv_scores "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("kf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'************************************ {} ************************************'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        trn_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" trn_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("valid_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("valid_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" clf_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lgb"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            train_matrix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("trn_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trn_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            valid_matrix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("val_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("val_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'boosting_type'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'gbdt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'objective'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'binary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'metric'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'auc'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'min_child_weight'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'num_leaves'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lambda_l2'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'feature_fraction'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bagging_fraction'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bagging_freq'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'learning_rate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'seed'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2020")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'nthread'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n_jobs'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("24")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'silent'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'verbose'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n            model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_matrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_sets"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_matrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_matrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" verbose_eval"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("early_stopping_rounds"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            val_pred "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("val_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_iteration"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("best_iteration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            test_pred "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_iteration"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("best_iteration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            \n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# print(list(sorted(zip(features, model.feature_importance("gain")), key=lambda x: x[1], reverse=True))[:20])')]),t._v("\n                \n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" clf_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xgb"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            train_matrix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DMatrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("trn_x "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trn_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            valid_matrix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DMatrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("val_x "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("val_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            \n            params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'booster'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'gbtree'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'objective'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'binary:logistic'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eval_metric'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'auc'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'gamma'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'min_child_weight'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'max_depth'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lambda'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'subsample'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'colsample_bytree'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'colsample_bylevel'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eta'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.04")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tree_method'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'exact'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'seed'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2020")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'nthread'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("36")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"silent"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n            \n            watchlist "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_matrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_matrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eval'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            \n            model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_matrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_boost_round"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" evals"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("watchlist"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" verbose_eval"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" early_stopping_rounds"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            val_pred  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_matrix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ntree_limit"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("best_ntree_limit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            test_pred "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_x "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ntree_limit"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("best_ntree_limit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                 \n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" clf_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cat"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'learning_rate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'depth'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'l2_leaf_reg'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bootstrap_type'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Bernoulli'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'od_type'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Iter'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'od_wait'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'random_seed'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("11")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'allow_writing_files'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n            \n            model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("iterations"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("trn_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" trn_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eval_set"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("val_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      cat_features"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" use_best_model"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" verbose"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            \n            val_pred  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("val_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            test_pred "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            \n        train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("valid_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" val_pred\n        test "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" test_pred "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" kf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("n_splits\n        cv_scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("roc_auc_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("val_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cv_scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"%s_scotrainre_list:"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" clf_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cv_scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"%s_score_mean:"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" clf_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cv_scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"%s_score_std:"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" clf_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("std"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cv_scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("lgb_model")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    lgb_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lgb_test "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cv_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lgb"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lgb"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" lgb_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lgb_test\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("xgb_model")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    xgb_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xgb_test "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cv_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xgb"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xgb"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" xgb_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xgb_test\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("cat_model")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    cat_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat_test "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cv_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("CatBoostRegressor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cat"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("lgb_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lgb_test "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lgb_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("************************************ 1 ************************************\nTraining until validation scores don't improve for 200 rounds\n[200]\ttraining's auc: 0.749225\tvalid_1's auc: 0.729679\n[400]\ttraining's auc: 0.765075\tvalid_1's auc: 0.730496\n[600]\ttraining's auc: 0.778745\tvalid_1's auc: 0.730435\nEarly stopping, best iteration is:\n[455]\ttraining's auc: 0.769202\tvalid_1's auc: 0.730686\n[0.7306859913754798]\n************************************ 2 ************************************\nTraining until validation scores don't improve for 200 rounds\n[200]\ttraining's auc: 0.749221\tvalid_1's auc: 0.731315\n[400]\ttraining's auc: 0.765117\tvalid_1's auc: 0.731658\n[600]\ttraining's auc: 0.778542\tvalid_1's auc: 0.731333\nEarly stopping, best iteration is:\n[407]\ttraining's auc: 0.765671\tvalid_1's auc: 0.73173\n[0.7306859913754798, 0.7317304414673989]\n************************************ 3 ************************************\nTraining until validation scores don't improve for 200 rounds\n[200]\ttraining's auc: 0.748436\tvalid_1's auc: 0.732775\n[400]\ttraining's auc: 0.764216\tvalid_1's auc: 0.733173\nEarly stopping, best iteration is:\n[386]\ttraining's auc: 0.763261\tvalid_1's auc: 0.733261\n[0.7306859913754798, 0.7317304414673989, 0.7332610441015461]\n************************************ 4 ************************************\nTraining until validation scores don't improve for 200 rounds\n[200]\ttraining's auc: 0.749631\tvalid_1's auc: 0.728327\n[400]\ttraining's auc: 0.765139\tvalid_1's auc: 0.728845\nEarly stopping, best iteration is:\n[286]\ttraining's auc: 0.756978\tvalid_1's auc: 0.728976\n[0.7306859913754798, 0.7317304414673989, 0.7332610441015461, 0.7289759386807912]\n************************************ 5 ************************************\nTraining until validation scores don't improve for 200 rounds\n[200]\ttraining's auc: 0.748414\tvalid_1's auc: 0.732727\n[400]\ttraining's auc: 0.763727\tvalid_1's auc: 0.733531\n[600]\ttraining's auc: 0.777489\tvalid_1's auc: 0.733566\nEarly stopping, best iteration is:\n[524]\ttraining's auc: 0.772372\tvalid_1's auc: 0.733772\n[0.7306859913754798, 0.7317304414673989, 0.7332610441015461, 0.7289759386807912, 0.7337723979789789]\nlgb_scotrainre_list: [0.7306859913754798, 0.7317304414673989, 0.7332610441015461, 0.7289759386807912, 0.7337723979789789]\nlgb_score_mean: 0.7316851627208389\nlgb_score_std: 0.0017424259863954693\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("testA_result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../testA_result.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("roc_auc_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("testA_result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'isDefault'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lgb_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("0.7290917729487896\n")])])]),s("h2",{attrs:{id:"_3-4-总结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-总结"}},[t._v("#")]),t._v(" 3.4 总结")]),t._v(" "),s("p",[t._v("特征工程是机器学习，甚至是深度学习中最为重要的一部分，在实际应用中往往也是所花费时间最多的一步。各种算法书中对特征工程部分的讲解往往少得可怜，因为特征工程和具体的数据结合的太紧密，很难系统地覆盖所有场景。本章主要是通过一些常用的方法来做介绍，例如缺失值异常值的处理方法详细对任何数据集来说都是适用的。但对于分箱等操作本章给出了具体的几种思路，需要读者自己探索。在特征工程中比赛和具体的应用还是有所不同的，在实际的金融风控评分卡制作过程中，由于强调特征的可解释性，特征分箱尤其重要。学有余力同学可以自行多尝试，希望大家在本节学习中有所收获。")])])}),[],!1,null,null,null);a.default=e.exports}}]);