(window.webpackJsonp=window.webpackJsonp||[]).push([[3],{360:function(t,s,a){t.exports=a.p+"assets/img/vanilla1.cf365937.png"},361:function(t,s,a){t.exports=a.p+"assets/img/vanilla_training.9913e716.png"},362:function(t,s,a){t.exports=a.p+"assets/img/transformerxl.c2358f22.png"},363:function(t,s,a){t.exports=a.p+"assets/img/xl_position.1ce2d685.png"},364:function(t,s,a){t.exports=a.p+"assets/img/xl_self_attention.f23d37af.png"},365:function(t,s,a){t.exports=a.p+"assets/img/step.984b2008.png"},402:function(t,s,a){"use strict";a.r(s);var e=a(42),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,e=t._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"transformerxl"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformerxl"}},[t._v("#")]),t._v(" TransformerXL")]),t._v(" "),e("h2",{attrs:{id:"introduction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),e("p",[t._v("在当前处理NLP的语言模型架构上，一般分两种：")]),t._v(" "),e("ul",[e("li",[t._v("基于RNN")]),t._v(" "),e("li",[t._v("基于Transformer")])]),t._v(" "),e("p",[t._v("然而，这两种模型框架都存在了一些缺点，在RNN模型中，存在了捕获长期依赖性上的一些问题，RNN体系的模型，像LSTM、GRU等，这些模型都是在RNN的基础上修改的，虽然一定程度上缓解了RNN的缺点，但是并不能很有效的去解决，当文本过长时，依然处理不了。而在Transformer中，并行输入数据，但是由于是深度神经网络，隐含层的参数过多，设备的计算能力有限，所以依然存在了长度上的限制。")]),t._v(" "),e("p",[t._v("因此在此之后，提出了一些模型，针对这两个问题进行改进，如Vanilla Transformer，Transformer XL，XLNet等，而现在要介绍的就是Vanilla Transformer和Transformer XL。")]),t._v(" "),e("h2",{attrs:{id:"vanilla-transformer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#vanilla-transformer"}},[t._v("#")]),t._v(" Vanilla Transformer")]),t._v(" "),e("p",[t._v("Vanilla Transformer是在Transformer提出之后的一个模型，它采用了Transformer作为一个特征提取器。但是Transformer是要固定长度的，如果比固定长度长，则可以通过一个简单又暴力的方法，直接将超过固定长度的部分截断。如果是比固定长度短，则通过padding的方式去填充。而在Vanilla Transformer在这里则是做了一个改动："),e("strong",[t._v("如果比固定长度长，则在超过固定长度的部分进行划分成不同的segment")]),t._v("。然后再不同的segment上进行训练。")]),t._v(" "),e("p",[t._v("那么问题来了，首先，segment之间的联系忽略了，两个segment之间是可能存在着某种联系的。另外，segment的划分是根据固定长度来划分的，这个划分边界不一定出现在句子结尾，也可能出现在一个词组的中间，这样的划分方式忽视了句子语义、语法信息，导致了segment中的片段并不是严格意义上的语言片段。")]),t._v(" "),e("p",[t._v("如下图所示，这就是划分成了两个segment的情况:")]),t._v(" "),e("p",[e("img",{attrs:{src:a(360),alt:"vanilla"}})]),t._v(" "),e("p",[t._v("为了可以结合两个segment之间的信息，vanilla采用了如下的训练方式：")]),t._v(" "),e("p",[e("img",{attrs:{src:a(361),alt:"vanilla training"}})]),t._v(" "),e("p",[t._v("我们可以看出，即便它划分成了一个个segment，但是在训练的时候，是在一个个token的往右移的训练的，"),e("strong",[t._v("在训练过程中，segment中使用同一个参数矩阵")]),t._v("。")]),t._v(" "),e("p",[t._v("还有一点的是，Vanilla Transformer采用的是一种"),e("strong",[t._v("绝对位置编码")]),t._v("的方式。这种方式会导致计算self-attention时带来一些影响。")]),t._v(" "),e("p",[t._v("以上便是Vanilla Transformer的模型。但是这样的模型同样也是会带来一些缺点的：")]),t._v(" "),e("ol",[e("li",[t._v("上下文的长度（依赖长度）会因为固定长度而受到限制")]),t._v(" "),e("li",[t._v("context fragment（上下文碎片）缺少依赖性，每个segment要重新开始训练")]),t._v(" "),e("li",[t._v("计算开销大")])]),t._v(" "),e("h2",{attrs:{id:"transformer-xl"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer-xl"}},[t._v("#")]),t._v(" Transformer XL")]),t._v(" "),e("p",[t._v("接下来便是这篇文章的主题Transformer XL了，Transformer XL将RNN与Vanilla Transformer结合起来，在每个segment上使用Transformer，而每个Transformer segment之间则是通过RNN去学习它们之间的依赖关系。")]),t._v(" "),e("p",[t._v("在Inference阶段，比传统的Transformer、Vanilla Transformer快了300~1800倍。")]),t._v(" "),e("p",[t._v("还有一点要说的是，在TransformerXL之中，只采用了Encoder模块。")]),t._v(" "),e("p",[t._v("Transformer XL依然划分了多个segment, 但是由于结合了RNN的优点，引入 了前一个segment的输出，即下图中绿色的线，另外，前一个segment的信息只参与了模型的前向计算，不参与反向传播。")]),t._v(" "),e("p",[e("img",{attrs:{src:a(362),alt:"Transformxl"}})]),t._v(" "),e("p",[t._v("接下来，我们不考虑multi-head，只考虑single-head的情况。下面这是计算公式：")]),t._v(" "),e("p",[t._v("![image-20200918154330386](/Users/Simonchan/Library/Application Support/typora-user-images/image-20200918154330386.png)")]),t._v(" "),e("p",[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mover",{attrs:{accent:"true"}},[e("mrow",[e("mi",[t._v("h")])],1),e("mo",[t._v("~")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\tilde{h}")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.9313em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.9313em","vertical-align":"0em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord accent"},[e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"mord textstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("h")])])]),e("span",{staticStyle:{top:"-0.61344em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"accent-body"},[e("span",[t._v("~")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("表示的是沿长度方向拼接，因为前一个segment是不计入梯度计算的，所以这里用SG表示，"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("τ")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\tau")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.43056em","vertical-align":"0em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.1132em"}},[t._v("τ")])])])]),t._v("表示前一个segment，"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("τ")]),e("mo",[t._v("+")]),e("mn",[t._v("1")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\tau+1")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.64444em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.72777em","vertical-align":"-0.08333em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.1132em"}},[t._v("τ")]),e("span",{staticClass:"mbin"},[t._v("+")]),e("span",{staticClass:"mord mathrm"},[t._v("1")])])])]),t._v("表示当前的segment，n-1表示的是前一层。h的维度是2L*d。")]),t._v(" "),e("h2",{attrs:{id:"transformer-xl-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer-xl-training"}},[t._v("#")]),t._v(" Transformer XL training")]),t._v(" "),e("p",[t._v("Transformer XL训练时，每个位置的隐向量除了自己位置的信息之外，都与前一个segment前一层的前L-1个位置的token都存在依赖关系。这可能有点绕，从下图可以看到，在第3层也就是最上面的一层的"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("x")]),e("mrow",[e("mn",[t._v("1")]),e("mn",[t._v("2")])],1)],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("x_{12}")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.58056em","vertical-align":"-0.15em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("1")]),e("span",{staticClass:"mord mathrm"},[t._v("2")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("计算时不仅跟当前segment的有关，还跟前面的segment的token有关。所以layer每往后深入，依赖长度就增加"),e("strong",[t._v("L-1")]),t._v("。")]),t._v(" "),e("p",[t._v("还有就是这样可以不必重复计算，因为缓存了上一个segment的结果。在实际操作时，保存尽可能多的segment到缓存。paper中做实验时只缓存了一个，作prediction的时候，缓存了多个。")]),t._v(" "),e("p",[t._v("![image-20200918155446765](/Users/Simonchan/Library/Application Support/typora-user-images/image-20200918155446765.png)")]),t._v(" "),e("h2",{attrs:{id:"transformer-xl位置编码"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer-xl位置编码"}},[t._v("#")]),t._v(" Transformer XL位置编码")]),t._v(" "),e("p",[t._v("在Transformer加入了位置编码，通过奇偶位置用sin、cos计算；但是在Transformer XL中，每个segment的transformer计算都加入这样的计算方式，会导致每个segment中位置编码信息一致，这样就无法确认位置信息。因此，Transformer XL放弃采用绝对位置编码机制，采用了一种相对位置编码，在计算hidden states时，依赖token的位置关系。")]),t._v(" "),e("p",[e("img",{attrs:{src:a(363),alt:"xl_positon"}})]),t._v(" "),e("h2",{attrs:{id:"transformer-xl的self-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer-xl的self-attention"}},[t._v("#")]),t._v(" Transformer XL的self-attention")]),t._v(" "),e("p",[t._v("Vanilla transformer 和 Transformer XL的self-attention计算对比：")]),t._v(" "),e("p",[e("img",{attrs:{src:a(364),alt:"xl_self_attention"}})]),t._v(" "),e("p",[t._v("这里的self-attention是在原本的基础上，做了extended扩展。先看原本的绝对位置编码，其实不难理解，这里是将原本的计算公式拆解成矩阵的形式。而在相对位置编码中，a是基于内容寻址，b是基于内容偏置，c是衡量全局内容偏置，d是全局位置偏置。")]),t._v(" "),e("p",[t._v("从上式中，会发现原本的"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("W")]),e("mi",[t._v("k")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("W_k")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.83333em","vertical-align":"-0.15em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("矩阵变成了多个不同的形式，这是因为跟相对位置有关，相当于分工明确了，使得矩阵可以学习到位置信息。而"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("u")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("u")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.43056em","vertical-align":"0em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord mathit"},[t._v("u")])])])]),t._v("和"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("v")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("v")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.43056em","vertical-align":"0em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("v")])])])]),t._v("则是原本"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("U")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("U")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.10903em"}},[t._v("U")])])])]),t._v("和"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("W")]),e("mi",[t._v("q")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("W_q")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.969438em","vertical-align":"-0.286108em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("q")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("相乘的后简化成这样的写法。")]),t._v(" "),e("h2",{attrs:{id:"步骤"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#步骤"}},[t._v("#")]),t._v(" 步骤")]),t._v(" "),e("p",[t._v("Transformer XL的计算步骤如下")]),t._v(" "),e("p",[e("img",{attrs:{src:a(365),alt:"image-20200918171753912"}})])])}),[],!1,null,null,null);s.default=n.exports}}]);