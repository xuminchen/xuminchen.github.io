(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{355:function(e,t,n){e.exports=n.p+"assets/img/seq2seq.5dcaabfe.png"},356:function(e,t,n){e.exports=n.p+"assets/img/attention-1.b50c7b9f.png"},405:function(e,t,n){"use strict";n.r(t);var o=n(42),r=Object(o.a)({},(function(){var e=this,t=e.$createElement,o=e._self._c||t;return o("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[o("h1",{attrs:{id:"sequence-to-sequence-with-attention"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#sequence-to-sequence-with-attention"}},[e._v("#")]),e._v(" Sequence to Sequence with Attention")]),e._v(" "),o("h2",{attrs:{id:"introduction"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),o("p",[e._v("一般来说，RNN和LSTM、GRU已经可以解决大量的NLP问题，但是在一些更专业的场景下，比如机器翻译，文本生成，单靠一个模型是无法有效地解决类似于语义理解的这些问题的，因此，提出了Seq2Seq的框架，由Encoder和Decoder模块一同完成这个任务。")]),e._v(" "),o("p",[e._v("如下图所示，Seq2seq的结构非常简单，左边的是Encoder的部分，右边的是Decoder的部分，以机器翻译的场景为例，我们希望我们的模型能够理解语句的意思并完成中译英的翻译，因此，我们在Encoder中输入中文，最后在Decoder中输出英文。")]),e._v(" "),o("p",[o("img",{attrs:{src:n(355),alt:"image-20200823155915855"}})]),e._v(" "),o("p",[e._v("整个过程并不难理解，两个模块其实内部都是RNN，在Encoder中，依次输入中文单词（字），输入完成后，输出最终的隐层向量，也就是"),o("strong",[e._v("语义向量")]),e._v("，这个向量可以理解为模型对输入句子语义的理解，之后输入到Decoder中。Decoder接收到这个语义向量，在英语的空间里面，用英语将这个语义表达出来（一个个单词输出），这就是整个Seq2Seq。")]),e._v(" "),o("h2",{attrs:{id:"attention"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#attention"}},[e._v("#")]),e._v(" Attention")]),e._v(" "),o("p",[e._v("虽然Seq2Seq与传统的机器翻译相比，有了非常大的提升，但是依然存在了一些瓶颈，比如怎么知道两句话中的对应关系呢？中文输入：我喜欢你，英文输出：I love you，模型是怎么才能知道喜欢在这里对应的是love呢？这就不得不提Attention机制了。")]),e._v(" "),o("p",[e._v("Attention机制如下图所示，输入的序列在经过Encoder后依然输出的是一个语义向量，但是这个语义向量不再和以前相同，这个语义向量中带有了单词在序列中占有的权重。具体的操作如下，序列中的token一一输入了Encoder之中，而输入后的每个hidden state都被存储起来，在Decoder中，首先"),o("strong",[e._v("起始符")]),e._v("输入进模型，输出第一个单词hidden state，这个hidden state与Encoder中存储起来的所有hidden state进行点积（可以是其他相似度计算操作），之后对所有的计算结果做Softmax，得到Decoder要输出的这个单词在句中的权重，将这个权重与decoder的hidden state做一个加权平均，得到了属于当前单词的语义向量，并输入到后续模型中。")]),e._v(" "),o("p",[o("img",{attrs:{src:n(356),alt:"attention"}})]),e._v(" "),o("blockquote",[o("p",[e._v("要注意，一开始传入Decoder中的hidden state，即在输入起始符的时候的隐层参数，和原始的seq2seq一样，是Encoder最后一层的输出。")])]),e._v(" "),o("p",[e._v("通过了attention机制，加强了Encoder和Decoder两者的联系，使得模型在输出的时候，更知道句子输出的时候是哪个单词。后续的很多模型都用到了")])])}),[],!1,null,null,null);t.default=r.exports}}]);