<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer | Simon&#39;s Blog</title>
    <meta name="generator" content="VuePress 1.5.4">
    <link rel="shoutcut icon" type="image/x-icon" href="logo.jpeg">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <meta name="description" content="">
    <link rel="preload" href="/assets/css/0.styles.bbe6d313.css" as="style"><link rel="preload" href="/assets/js/app.15c63893.js" as="script"><link rel="preload" href="/assets/js/2.83a52e8a.js" as="script"><link rel="preload" href="/assets/js/5.ed31ef13.js" as="script"><link rel="prefetch" href="/assets/js/10.273339ea.js"><link rel="prefetch" href="/assets/js/11.f34c48bf.js"><link rel="prefetch" href="/assets/js/12.acca6533.js"><link rel="prefetch" href="/assets/js/13.277cc9aa.js"><link rel="prefetch" href="/assets/js/14.dab476be.js"><link rel="prefetch" href="/assets/js/15.b0e7e4e6.js"><link rel="prefetch" href="/assets/js/16.3dc194be.js"><link rel="prefetch" href="/assets/js/17.72a2f9f4.js"><link rel="prefetch" href="/assets/js/18.adf1b62c.js"><link rel="prefetch" href="/assets/js/19.26404820.js"><link rel="prefetch" href="/assets/js/20.6e7aad67.js"><link rel="prefetch" href="/assets/js/21.74c7398f.js"><link rel="prefetch" href="/assets/js/22.3748e0d7.js"><link rel="prefetch" href="/assets/js/23.212a90bf.js"><link rel="prefetch" href="/assets/js/24.2ac39710.js"><link rel="prefetch" href="/assets/js/25.f02e60b9.js"><link rel="prefetch" href="/assets/js/26.e4d5dd86.js"><link rel="prefetch" href="/assets/js/27.0fd34753.js"><link rel="prefetch" href="/assets/js/28.700db19a.js"><link rel="prefetch" href="/assets/js/29.56b5f8da.js"><link rel="prefetch" href="/assets/js/3.ddb8398f.js"><link rel="prefetch" href="/assets/js/30.32ee25d2.js"><link rel="prefetch" href="/assets/js/31.d2e27490.js"><link rel="prefetch" href="/assets/js/32.7b2392e4.js"><link rel="prefetch" href="/assets/js/33.2e320e29.js"><link rel="prefetch" href="/assets/js/34.7b6d6140.js"><link rel="prefetch" href="/assets/js/35.fe23083b.js"><link rel="prefetch" href="/assets/js/36.5faf7922.js"><link rel="prefetch" href="/assets/js/37.e22ef41e.js"><link rel="prefetch" href="/assets/js/38.2b6f7ae4.js"><link rel="prefetch" href="/assets/js/39.8e26d55a.js"><link rel="prefetch" href="/assets/js/4.6545323a.js"><link rel="prefetch" href="/assets/js/40.67b90976.js"><link rel="prefetch" href="/assets/js/6.bf0684a8.js"><link rel="prefetch" href="/assets/js/7.2dba22aa.js"><link rel="prefetch" href="/assets/js/8.e2ad90ae.js"><link rel="prefetch" href="/assets/js/9.b2ec3081.js">
    <link rel="stylesheet" href="/assets/css/0.styles.bbe6d313.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="logo.jpeg" alt="Simon's Blog" class="logo"> <span class="site-name can-hide">Simon's Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/notebook/" class="nav-link router-link-active">
  Blog
</a></div><div class="nav-item"><a href="/blog/competition/" class="nav-link">
  Competition
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/blog/notebook/" class="nav-link router-link-active">
  Blog
</a></div><div class="nav-item"><a href="/blog/competition/" class="nav-link">
  Competition
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Blog</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/blog/notebook/Introduction.html" class="sidebar-link">Introduction</a></li><li><a href="/blog/notebook/文本预处理.html" class="sidebar-link">文本预处理</a></li><li><a href="/blog/notebook/LR.html" class="sidebar-link">Logistic Regression</a></li><li><a href="/blog/notebook/Regularization.html" class="sidebar-link">Regularization</a></li><li><a href="/blog/notebook/SVM.html" class="sidebar-link">Support Vector Machine</a></li><li><a href="/blog/notebook/HMM.html" class="sidebar-link">Hidden Markov Model</a></li><li><a href="/blog/notebook/CRF.html" class="sidebar-link">Conditional Random Field</a></li><li><a href="/blog/notebook/decisiontree.html" class="sidebar-link">决策树系列</a></li><li><a href="/blog/notebook/Word2vec.html" class="sidebar-link">Word2vec</a></li><li><a href="/blog/notebook/Glove.html" class="sidebar-link">Glove</a></li><li><a href="/blog/notebook/RNN.html" class="sidebar-link">RNN</a></li><li><a href="/blog/notebook/LSTM.html" class="sidebar-link">LSTM</a></li><li><a href="/blog/notebook/GRU.html" class="sidebar-link">GRU</a></li><li><a href="/blog/notebook/Seq2Seq_with_attention.html" class="sidebar-link">Sequence to Sequence with Attention</a></li><li><a href="/blog/notebook/Transformer.html" aria-current="page" class="active sidebar-link">Transformer</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/blog/notebook/Transformer.html#introduction" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/Transformer.html#self-attention-layer" class="sidebar-link">Self Attention layer</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/Transformer.html#multi-head-attention" class="sidebar-link">Multi-head attention</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/Transformer.html#position-embedding" class="sidebar-link">Position Embedding</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/Transformer.html#decoder-attention-masking" class="sidebar-link">Decoder attention masking</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/Transformer.html#整体结构" class="sidebar-link">整体结构</a></li></ul></li><li><a href="/blog/notebook/TransformerXL.html" class="sidebar-link">TransformerXL</a></li><li><a href="/blog/notebook/BERT.html" class="sidebar-link">BERT</a></li><li><a href="/blog/notebook/BLEU.html" class="sidebar-link">BLEU</a></li><li><a href="/blog/notebook/HNSW.html" class="sidebar-link">HNSW</a></li><li><a href="/blog/notebook/study_material.html" class="sidebar-link">学习资料</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="transformer"><a href="#transformer" class="header-anchor">#</a> Transformer</h1> <h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <p>什么是Transformer？能否介绍一下Transformer？这是面试的时候常会问到的问题，关于这个问题，我的答案是这样的：</p> <p>Transformer是一个基于Seq2Seq的Encoder-Decoder之上的深度神经网络，它用multi-head的self-Attention结构取代了原本Seq2seq之中采用的RNN或LSTM的结构，输入进模型的除了token的embedding之外，还有position embedding位置编码，该位置基于奇偶位置用sin、cos去计算位置编码的值。位置编码和word embedding输入模型的Encoder模块。Transformer的Encoder部分是由6个Encoder layer叠加而成，每个Encoder layer包括了一个self-attention layer和feed-forward layer组成，之后再传到下一个encoder layer去。其中，在self-attention和feed-forward 两个layer后都有一个add&amp;norm，作残差相加和归一化的计算。之后Encoder输出的是一个语义向量，输入到Decoder模块中。Decoder模块同样是由6个decoder layer组成，而每个decoder layer与encoder layer稍微有些不同，每个decoder layer包括一个self-attention、multi-head encoder-decoder layer和feed-forward layer，其中self-attention用的是encoder部分中的K矩阵和V矩阵，另外用了一个attention mask，把还没轮到的token遮掩住。而encoder-decoder attention部分是和传统的attention计算方式是一样的。同样，decoder layer的每一part之间存在着add&amp;norm模块。</p> <p><img src="/assets/img/transformer.dfc59528.png" alt="Transformer"></p> <p><img src="/assets/img/structure.188eba9d.png" alt="Encoder-Decoder"></p> <h2 id="self-attention-layer"><a href="#self-attention-layer" class="header-anchor">#</a> Self Attention layer</h2> <p>self attention，说是attention，实际上和attention并没啥关系。self attention关注的是输入中的token与token之间的潜在联系。self attention的计算方式如下，首先position embedding和word embedding相加后，作为输入进入到模型，与三个权重矩阵相乘，得到q，k，v三个向量，q与当前所有输入的k矩阵进行<strong>点积</strong>计算（可以用其他相似度计算方法），之后将这些score进行softmax计算，作为每个token的在当前输入对当前token的意义的比例，也就是权重，之后将这个权重与其他词的v作sum product，得到该token的输出，进入后续的步骤。</p> <p><img src="/assets/img/self-attention.b1c69273.png" alt="self-attention"></p> <p>另外，self attention 是<strong>并行</strong>输入其他单词的，比起rnn、lstm，更能捕获长时间序列的文本依赖关系。</p> <p>还有一点要提的是，在softmax之前，对每个score都要除以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt {d_k}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.8572200000000001em;"></span><span class="strut bottom" style="height:1.04em;vertical-align:-0.18278em;"></span><span class="base textstyle uncramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">√</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span>，这表示的是每个embedding的维度，因为用多头注意力之后，原本的模型方差会由1变成更大的数值，论文中，实验通过除以这样一个数值，去缩小了方差。</p> <h2 id="multi-head-attention"><a href="#multi-head-attention" class="header-anchor">#</a> Multi-head attention</h2> <p>从名字上可以看出，multihead，多个头。在这里的意思是，通过多个Q，K，V矩阵，完成self-attention的运算。计算完之后，横向将结果拼接起来，然后再乘以一个矩阵，进行降维，得到输出的向量。</p> <p>比如说self-attention层输出的是一个1*d维的向量，经过了k个head，所以会得到一个1*kd的向量，这时候再乘以一个kd*d的矩阵，将向量变回1*d的维度。</p> <p>**为什么要用multi-head attention？**这个也是面试的时候经常会被问到的一个问题，事实上，论文里也没有说这个的意义是什么，数学上也并没有详细的推理到，但是从直观意义上去理解，多个头可以关注到不同的子空间，增加了模型的泛化能力。</p> <h2 id="position-embedding"><a href="#position-embedding" class="header-anchor">#</a> Position Embedding</h2> <p>由于训练Transformer模型时，序列输入采用并行方式，因此缺少单词的位置信息，通过在Transformer的输入中加入单词位置编码信息，使Transformer能够识别语句中单词的位置关系。</p> <p><strong>位置编码（positional encoding）</strong>：位置编码向量与词向量维度相同，\text{max_seq_len} \times \text{embedding_dim}。</p> <p>Transformer原文中使用正、余弦函数的线性变换对单词位置编码：</p>
\text{PE}_{pos, 2i} = \sin \left( \frac{pos}{10000^{2i / d_{\text{model}}}} \right) \\ 
\text{PE}_{(pos,2i + 1)} = \cos \left( \frac{pos}{10000^{2i / d_{\text{model}}}} \right)
<p>其中，pos \in [0, \text{max_seq_len})表示单词在语句中的位置，i \in [0, \text{embedding_dim})表示词向量维度。位置编码函数的波长在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mn>2</mn><mi>π</mi><mo separator="true">,</mo><mn>1</mn><mn>0</mn><mn>0</mn><mn>0</mn><mn>0</mn><mo>×</mo><mn>2</mn><mi>π</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">[2 \pi, 10000 \times 2 \pi]</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">[</span><span class="mord mathrm">2</span><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mbin">×</span><span class="mord mathrm">2</span><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mclose">]</span></span></span></span>区间内变化，语句中每一个单词位置沿词向量维度由周期不同的正、余弦函数交替取值组合，生成独一纹理信息，从而使模型学到位置间的依赖关系和自然语言的时序特性。</p> <h2 id="decoder-attention-masking"><a href="#decoder-attention-masking" class="header-anchor">#</a> Decoder attention masking</h2> <p>Decoder部分的self-attention layer，这里计算时候，不再用三个权重矩阵生成q、k、v三个向量，而是只需要一个Q矩阵计算出query向量即可，而K，V矩阵则是来自Encoder的K，V矩阵。</p> <p>另外，self-attention是并行输入的，但是Decoder的生成步骤是一个接一个单词的输出，因此，需要将未出现的单词mask掉，操作很简单，将未出现的词语设置成负无穷的数即可，使其不产生作用。</p> <h2 id="整体结构"><a href="#整体结构" class="header-anchor">#</a> 整体结构</h2> <p>Transformer编码器基本单元由两个子层组成：第一个子层实现多头**自注意力（self-attention）**机制（Multi-Head Attention）；第二个子层实现全连接前馈网络。计算过程如下：</p> <ol><li><strong>词向量与位置编码</strong></li></ol>
X = \text{EmbeddingLookup}(X) + \text{PositionalEncoding} \tag{2}

X \in \mathbb{R}^{\text{batch_size} \times \text{seq_len} \times \text{embedding_dim}}

<ol start="2"><li><strong>自注意力机制</strong></li></ol> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>=</mo><mtext><mi mathvariant="normal">L</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mtext><mo>(</mo><mi>X</mi><mo>)</mo><mo>=</mo><mi>X</mi><msub><mi>W</mi><mrow><mi>Q</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q = \text{Linear}(X) = X W_{Q}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">Q</span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">L</span><span class="mord mathrm">i</span><span class="mord mathrm">n</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">Q</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p>
K = \text{Linear}(X) = XW_{K} \tag{3}

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi><mo>=</mo><mtext><mi mathvariant="normal">L</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mtext><mo>(</mo><mi>X</mi><mo>)</mo><mo>=</mo><mi>X</mi><msub><mi>W</mi><mrow><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">V = \text{Linear}(X) = XW_{V}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">L</span><span class="mord mathrm">i</span><span class="mord mathrm">n</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.22222em;">V</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p>
X_{\text{attention}} = \text{SelfAttention}(Q, K, V) \tag{4}

<ol start="3"><li><strong>层归一化、残差连接</strong></li></ol>
X_{\text{attention}} = \text{LayerNorm}(X_{\text{attention}}) \tag{5}

X_{\text{attention}} = X + X_{\text{attention}} \tag{6}

<ol start="4"><li><strong>前馈网络</strong></li></ol>
X_{\text{hidden}} = \text{Linear}(\text{Activate}(\text{Linear}(X_{\text{attention}}))) \tag{7}

<ol start="5"><li><strong>层归一化、残差连接</strong></li></ol> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mrow><mtext><mi mathvariant="normal">h</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub><mo>=</mo><mtext><mi mathvariant="normal">L</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">m</mi></mtext><mo>(</mo><msub><mi>X</mi><mrow><mtext><mi mathvariant="normal">h</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">X_{\text{hidden}} = \text{LayerNorm}(X_{\text{hidden}})
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">h</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">d</span><span class="mord mathrm">e</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">L</span><span class="mord mathrm">a</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm">N</span><span class="mord mathrm">o</span><span class="mord mathrm">r</span><span class="mord mathrm">m</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">h</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">d</span><span class="mord mathrm">e</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span></p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mrow><mtext><mi mathvariant="normal">h</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub><mo>+</mo><msub><mi>X</mi><mrow><mtext><mi mathvariant="normal">h</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow><annotation encoding="application/x-tex">X_{\text{hidden}} = X_{\text{attention}} + X_{\text{hidden}}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">h</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">d</span><span class="mord mathrm">e</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">a</span><span class="mord mathrm">t</span><span class="mord mathrm">t</span><span class="mord mathrm">e</span><span class="mord mathrm">n</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">h</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">d</span><span class="mord mathrm">e</span><span class="mord mathrm">n</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p>
X_{\text{hidden}} \in \mathbb{R}^{\text{batch_size} \times \text{seq_len} \times \text{embedding_dim}}

<p>Transformer编码器由<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">N = 6</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mrel">=</span><span class="mord mathrm">6</span></span></span></span>个编码器基本单元组成。</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/blog/notebook/Seq2Seq_with_attention.html" class="prev">
        Sequence to Sequence with Attention
      </a></span> <span class="next"><a href="/blog/notebook/TransformerXL.html">
        TransformerXL
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.15c63893.js" defer></script><script src="/assets/js/2.83a52e8a.js" defer></script><script src="/assets/js/5.ed31ef13.js" defer></script>
  </body>
</html>
