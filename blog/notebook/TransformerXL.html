<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>TransformerXL | Simon&#39;s Blog</title>
    <meta name="generator" content="VuePress 1.5.4">
    <link rel="shoutcut icon" type="image/x-icon" href="logo.jpeg">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <meta name="description" content="">
    <link rel="preload" href="/assets/css/0.styles.bbe6d313.css" as="style"><link rel="preload" href="/assets/js/app.15c63893.js" as="script"><link rel="preload" href="/assets/js/2.83a52e8a.js" as="script"><link rel="preload" href="/assets/js/3.ddb8398f.js" as="script"><link rel="prefetch" href="/assets/js/10.273339ea.js"><link rel="prefetch" href="/assets/js/11.f34c48bf.js"><link rel="prefetch" href="/assets/js/12.acca6533.js"><link rel="prefetch" href="/assets/js/13.277cc9aa.js"><link rel="prefetch" href="/assets/js/14.dab476be.js"><link rel="prefetch" href="/assets/js/15.b0e7e4e6.js"><link rel="prefetch" href="/assets/js/16.3dc194be.js"><link rel="prefetch" href="/assets/js/17.72a2f9f4.js"><link rel="prefetch" href="/assets/js/18.adf1b62c.js"><link rel="prefetch" href="/assets/js/19.26404820.js"><link rel="prefetch" href="/assets/js/20.6e7aad67.js"><link rel="prefetch" href="/assets/js/21.74c7398f.js"><link rel="prefetch" href="/assets/js/22.3748e0d7.js"><link rel="prefetch" href="/assets/js/23.212a90bf.js"><link rel="prefetch" href="/assets/js/24.2ac39710.js"><link rel="prefetch" href="/assets/js/25.f02e60b9.js"><link rel="prefetch" href="/assets/js/26.e4d5dd86.js"><link rel="prefetch" href="/assets/js/27.0fd34753.js"><link rel="prefetch" href="/assets/js/28.700db19a.js"><link rel="prefetch" href="/assets/js/29.56b5f8da.js"><link rel="prefetch" href="/assets/js/30.32ee25d2.js"><link rel="prefetch" href="/assets/js/31.d2e27490.js"><link rel="prefetch" href="/assets/js/32.7b2392e4.js"><link rel="prefetch" href="/assets/js/33.2e320e29.js"><link rel="prefetch" href="/assets/js/34.7b6d6140.js"><link rel="prefetch" href="/assets/js/35.fe23083b.js"><link rel="prefetch" href="/assets/js/36.5faf7922.js"><link rel="prefetch" href="/assets/js/37.e22ef41e.js"><link rel="prefetch" href="/assets/js/38.2b6f7ae4.js"><link rel="prefetch" href="/assets/js/39.8e26d55a.js"><link rel="prefetch" href="/assets/js/4.6545323a.js"><link rel="prefetch" href="/assets/js/40.67b90976.js"><link rel="prefetch" href="/assets/js/5.ed31ef13.js"><link rel="prefetch" href="/assets/js/6.bf0684a8.js"><link rel="prefetch" href="/assets/js/7.2dba22aa.js"><link rel="prefetch" href="/assets/js/8.e2ad90ae.js"><link rel="prefetch" href="/assets/js/9.b2ec3081.js">
    <link rel="stylesheet" href="/assets/css/0.styles.bbe6d313.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="logo.jpeg" alt="Simon's Blog" class="logo"> <span class="site-name can-hide">Simon's Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/notebook/" class="nav-link router-link-active">
  Blog
</a></div><div class="nav-item"><a href="/blog/competition/" class="nav-link">
  Competition
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/blog/notebook/" class="nav-link router-link-active">
  Blog
</a></div><div class="nav-item"><a href="/blog/competition/" class="nav-link">
  Competition
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Blog</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/blog/notebook/Introduction.html" class="sidebar-link">Introduction</a></li><li><a href="/blog/notebook/文本预处理.html" class="sidebar-link">文本预处理</a></li><li><a href="/blog/notebook/LR.html" class="sidebar-link">Logistic Regression</a></li><li><a href="/blog/notebook/Regularization.html" class="sidebar-link">Regularization</a></li><li><a href="/blog/notebook/SVM.html" class="sidebar-link">Support Vector Machine</a></li><li><a href="/blog/notebook/HMM.html" class="sidebar-link">Hidden Markov Model</a></li><li><a href="/blog/notebook/CRF.html" class="sidebar-link">Conditional Random Field</a></li><li><a href="/blog/notebook/decisiontree.html" class="sidebar-link">决策树系列</a></li><li><a href="/blog/notebook/Word2vec.html" class="sidebar-link">Word2vec</a></li><li><a href="/blog/notebook/Glove.html" class="sidebar-link">Glove</a></li><li><a href="/blog/notebook/RNN.html" class="sidebar-link">RNN</a></li><li><a href="/blog/notebook/LSTM.html" class="sidebar-link">LSTM</a></li><li><a href="/blog/notebook/GRU.html" class="sidebar-link">GRU</a></li><li><a href="/blog/notebook/Seq2Seq_with_attention.html" class="sidebar-link">Sequence to Sequence with Attention</a></li><li><a href="/blog/notebook/Transformer.html" class="sidebar-link">Transformer</a></li><li><a href="/blog/notebook/TransformerXL.html" aria-current="page" class="active sidebar-link">TransformerXL</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/blog/notebook/TransformerXL.html#introduction" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/TransformerXL.html#vanilla-transformer" class="sidebar-link">Vanilla Transformer</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/TransformerXL.html#transformer-xl" class="sidebar-link">Transformer XL</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/TransformerXL.html#transformer-xl-training" class="sidebar-link">Transformer XL training</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/TransformerXL.html#transformer-xl位置编码" class="sidebar-link">Transformer XL位置编码</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/TransformerXL.html#transformer-xl的self-attention" class="sidebar-link">Transformer XL的self-attention</a></li><li class="sidebar-sub-header"><a href="/blog/notebook/TransformerXL.html#步骤" class="sidebar-link">步骤</a></li></ul></li><li><a href="/blog/notebook/BERT.html" class="sidebar-link">BERT</a></li><li><a href="/blog/notebook/BLEU.html" class="sidebar-link">BLEU</a></li><li><a href="/blog/notebook/HNSW.html" class="sidebar-link">HNSW</a></li><li><a href="/blog/notebook/study_material.html" class="sidebar-link">学习资料</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="transformerxl"><a href="#transformerxl" class="header-anchor">#</a> TransformerXL</h1> <h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <p>在当前处理NLP的语言模型架构上，一般分两种：</p> <ul><li>基于RNN</li> <li>基于Transformer</li></ul> <p>然而，这两种模型框架都存在了一些缺点，在RNN模型中，存在了捕获长期依赖性上的一些问题，RNN体系的模型，像LSTM、GRU等，这些模型都是在RNN的基础上修改的，虽然一定程度上缓解了RNN的缺点，但是并不能很有效的去解决，当文本过长时，依然处理不了。而在Transformer中，并行输入数据，但是由于是深度神经网络，隐含层的参数过多，设备的计算能力有限，所以依然存在了长度上的限制。</p> <p>因此在此之后，提出了一些模型，针对这两个问题进行改进，如Vanilla Transformer，Transformer XL，XLNet等，而现在要介绍的就是Vanilla Transformer和Transformer XL。</p> <h2 id="vanilla-transformer"><a href="#vanilla-transformer" class="header-anchor">#</a> Vanilla Transformer</h2> <p>Vanilla Transformer是在Transformer提出之后的一个模型，它采用了Transformer作为一个特征提取器。但是Transformer是要固定长度的，如果比固定长度长，则可以通过一个简单又暴力的方法，直接将超过固定长度的部分截断。如果是比固定长度短，则通过padding的方式去填充。而在Vanilla Transformer在这里则是做了一个改动：<strong>如果比固定长度长，则在超过固定长度的部分进行划分成不同的segment</strong>。然后再不同的segment上进行训练。</p> <p>那么问题来了，首先，segment之间的联系忽略了，两个segment之间是可能存在着某种联系的。另外，segment的划分是根据固定长度来划分的，这个划分边界不一定出现在句子结尾，也可能出现在一个词组的中间，这样的划分方式忽视了句子语义、语法信息，导致了segment中的片段并不是严格意义上的语言片段。</p> <p>如下图所示，这就是划分成了两个segment的情况:</p> <p><img src="/assets/img/vanilla1.cf365937.png" alt="vanilla"></p> <p>为了可以结合两个segment之间的信息，vanilla采用了如下的训练方式：</p> <p><img src="/assets/img/vanilla_training.9913e716.png" alt="vanilla training"></p> <p>我们可以看出，即便它划分成了一个个segment，但是在训练的时候，是在一个个token的往右移的训练的，<strong>在训练过程中，segment中使用同一个参数矩阵</strong>。</p> <p>还有一点的是，Vanilla Transformer采用的是一种<strong>绝对位置编码</strong>的方式。这种方式会导致计算self-attention时带来一些影响。</p> <p>以上便是Vanilla Transformer的模型。但是这样的模型同样也是会带来一些缺点的：</p> <ol><li>上下文的长度（依赖长度）会因为固定长度而受到限制</li> <li>context fragment（上下文碎片）缺少依赖性，每个segment要重新开始训练</li> <li>计算开销大</li></ol> <h2 id="transformer-xl"><a href="#transformer-xl" class="header-anchor">#</a> Transformer XL</h2> <p>接下来便是这篇文章的主题Transformer XL了，Transformer XL将RNN与Vanilla Transformer结合起来，在每个segment上使用Transformer，而每个Transformer segment之间则是通过RNN去学习它们之间的依赖关系。</p> <p>在Inference阶段，比传统的Transformer、Vanilla Transformer快了300~1800倍。</p> <p>还有一点要说的是，在TransformerXL之中，只采用了Encoder模块。</p> <p>Transformer XL依然划分了多个segment, 但是由于结合了RNN的优点，引入 了前一个segment的输出，即下图中绿色的线，另外，前一个segment的信息只参与了模型的前向计算，不参与反向传播。</p> <p><img src="/assets/img/transformerxl.c2358f22.png" alt="Transformxl"></p> <p>接下来，我们不考虑multi-head，只考虑single-head的情况。下面这是计算公式：</p> <p>![image-20200918154330386](/Users/Simonchan/Library/Application Support/typora-user-images/image-20200918154330386.png)</p> <p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>h</mi></mrow><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{h}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.9313em;"></span><span class="strut bottom" style="height:0.9313em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit">h</span></span></span><span style="top:-0.61344em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>表示的是沿长度方向拼接，因为前一个segment是不计入梯度计算的，所以这里用SG表示，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span></span></span></span>表示前一个segment，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau+1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.1132em;">τ</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span>表示当前的segment，n-1表示的是前一层。h的维度是2L*d。</p> <h2 id="transformer-xl-training"><a href="#transformer-xl-training" class="header-anchor">#</a> Transformer XL training</h2> <p>Transformer XL训练时，每个位置的隐向量除了自己位置的信息之外，都与前一个segment前一层的前L-1个位置的token都存在依赖关系。这可能有点绕，从下图可以看到，在第3层也就是最上面的一层的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{12}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">1</span><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>计算时不仅跟当前segment的有关，还跟前面的segment的token有关。所以layer每往后深入，依赖长度就增加<strong>L-1</strong>。</p> <p>还有就是这样可以不必重复计算，因为缓存了上一个segment的结果。在实际操作时，保存尽可能多的segment到缓存。paper中做实验时只缓存了一个，作prediction的时候，缓存了多个。</p> <p>![image-20200918155446765](/Users/Simonchan/Library/Application Support/typora-user-images/image-20200918155446765.png)</p> <h2 id="transformer-xl位置编码"><a href="#transformer-xl位置编码" class="header-anchor">#</a> Transformer XL位置编码</h2> <p>在Transformer加入了位置编码，通过奇偶位置用sin、cos计算；但是在Transformer XL中，每个segment的transformer计算都加入这样的计算方式，会导致每个segment中位置编码信息一致，这样就无法确认位置信息。因此，Transformer XL放弃采用绝对位置编码机制，采用了一种相对位置编码，在计算hidden states时，依赖token的位置关系。</p> <p><img src="/assets/img/xl_position.1ce2d685.png" alt="xl_positon"></p> <h2 id="transformer-xl的self-attention"><a href="#transformer-xl的self-attention" class="header-anchor">#</a> Transformer XL的self-attention</h2> <p>Vanilla transformer 和 Transformer XL的self-attention计算对比：</p> <p><img src="/assets/img/xl_self_attention.f23d37af.png" alt="xl_self_attention"></p> <p>这里的self-attention是在原本的基础上，做了extended扩展。先看原本的绝对位置编码，其实不难理解，这里是将原本的计算公式拆解成矩阵的形式。而在相对位置编码中，a是基于内容寻址，b是基于内容偏置，c是衡量全局内容偏置，d是全局位置偏置。</p> <p>从上式中，会发现原本的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>矩阵变成了多个不同的形式，这是因为跟相对位置有关，相当于分工明确了，使得矩阵可以学习到位置信息。而<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">u</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">v</span></span></span></span>则是原本<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">U</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>相乘的后简化成这样的写法。</p> <h2 id="步骤"><a href="#步骤" class="header-anchor">#</a> 步骤</h2> <p>Transformer XL的计算步骤如下</p> <p><img src="/assets/img/step.984b2008.png" alt="image-20200918171753912"></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/blog/notebook/Transformer.html" class="prev">
        Transformer
      </a></span> <span class="next"><a href="/blog/notebook/BERT.html">
        BERT
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.15c63893.js" defer></script><script src="/assets/js/2.83a52e8a.js" defer></script><script src="/assets/js/3.ddb8398f.js" defer></script>
  </body>
</html>
